[(109条消息) VMware安装win10系统_渣渣苏的博客-CSDN博客](https://blog.csdn.net/su2231595742/article/details/123915481)

[(134条消息) VMware虚拟机部署k8s集群_异乡人hl的博客-CSDN博客_vmware部署k8s](https://blog.csdn.net/qq_41860461/article/details/122418639)

[(134条消息) Hadoop入门(一)——CentOS7下载+VM上安装（手动分区）图文步骤详解(2021)_哨哨可不苕的博客-CSDN博客](https://blog.csdn.net/m0_46413065/article/details/114667174)

# VMware网络配置参考：

[尚硅谷大数据Hadoop教程（Hadoop 3.x安装搭建到集群调优）_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1Qp4y1n7EN?p=20)

## 2. 配置IP地址

1. **VMware ip**

   1. ![image-20220608151239251](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220608151239251.png)
   2. ![image-20220608151331066](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220608151331066.png)
   3. ![image-20220608151435062](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220608151435062.png)
   4. ![image-20220608151629872](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220608151629872.png)
   5. ![image-20220608151701212](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220608151701212.png)

2. **win10 ip**

   1. ![image-20220608151834210](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220608151834210.png)

3. **虚拟机系统 ip**

   1. 查看默认使用的网卡

      ![image-20220608163223347](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220608163223347.png)

      使用的是ens33

   2. 修改ip地址

      1. vi /etc/sysconfig/network-scripts/ifcfg-ens33

         ![image-20220608152411520](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220608152411520.png)

      2. 配置主机名称 vi /etc/hostname

         1. ![image-20220608152550381](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220608152550381.png)

      3. 主机名称映射

         1. ip地址：192.168.10.100  

            1. 之后写代码的时候，任何需要使用ip地址的地方都要用192.168.10.100，如果有一天ip地址发生变化，则系统中全部的都要修改
            2. 使用k8s-node01代替 192.168.10.100 程序中直接使用k8s-node01

         2. vi /etc/hosts

            ![image-20220608152926586](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220608152926586.png)

         

4. 重启系统 reboot

   1. ip addr查看ip地址

      ![image-20220608153231867](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220608153231867.png)

   2. ping www.baidu.com 查看是否配置成功

      ![image-20220608153323748](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220608153323748.png)

### 问题：

#### 1. 进程无法访问

![image-20220609202934669](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220609202934669.png)

![image-20220609205231985](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220609205231985.png)

#### 2. 重启电脑后，虚拟机ping不通

![image-20220609205207421](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220609205207421.png)

#### 3. Operating System Not Found

http://t.zoukankan.com/y1ran-p-12132311.html

#### 4. 断电后出现以下报错

![image-20220623091944739](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220623091944739.png)

解决方式：

https://blog.csdn.net/qq_30665009/article/details/124763345

## 3. 配置K8s

[Java项目《谷粒商城》Java架构师 | 微服务 | 大型电商项目_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1np4y1C7Yf?p=346&spm_id_from=pageDriver)

### 1. 安装docker

[Linux开发环境配置（Docker） · 语雀 (yuque.com)](https://www.yuque.com/zhangshuaiyin/guli-mall/lb4zw1)

#### 问题：

#### 1. yum - install yum-utils 安装失败

![image-20220609094031049](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220609094031049.png)

[(134条消息) sudo yum install -y yum-utils 时出错_i小喇叭的博客-CSDN博客_yum-utils安装报错](https://blog.csdn.net/weixin_42230797/article/details/122909935)

![image-20220609093958524](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220609093958524.png)

#### 2. This system is not registered with an entitlement server

![image-20220609100623026](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220609100623026.png)

###  2. 安装kubeadm，kubelet和kubectl

```
yum list|grep kube
```

安装

```shell
yum install -y kubelet-1.17.3 kubeadm-1.17.3 kubectl-1.17.3
```

开机启动

```shell
systemctl enable kubelet # 开机启动
systemctl start kubelet # 启动kubelet
或者：systemctl enable kubelet && systemctl start kubelet # 同时执行两条命令
```

查看kubelet的状态：

```
systemctl status kubelet
```

![image-20220609103050467](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220609103050467.png)

原因：未配置号。

查看kubelet版本：

```shell
[root@k8s-node2 ~]# kubelet --version
Kubernetes v1.17.3
```

### 3. 初始化master节点

#### 1. master需要用的docker镜像

**shell脚本**

```shell
#!/bin/bash

images=(
	kube-apiserver:v1.17.3
    kube-proxy:v1.17.3
	kube-controller-manager:v1.17.3
	kube-scheduler:v1.17.3
	coredns:1.6.5
	etcd:3.4.3-0
    pause:3.1
)

for imageName in ${images[@]} ; do
    docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName
#   docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName  k8s.gcr.io/$imageName
done
```

![image-20220609105059036](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220609105059036.png)

rwx：可读，可写，可执行![image-20220609105132765](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220609105132765.png)

运行shell脚本：`./master_images.sh `

#### 2. 初始化命令

1. **通过kubeadm初始化k8s**

```shell
执行以下命令：
kubeadm init --apiserver-advertise-address=192.168.10.100 --image-repository registry.cn-hangzhou.aliyuncs.com/google_containers --kubernetes-version   v1.17.3 --service-cidr=10.96.0.0/16  --pod-network-cidr=10.244.0.0/16
```

![image-20220609110832482](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220609110832482.png)

相关命令解读：[kubeadm init | Kubernetes](https://kubernetes.io/zh/docs/reference/setup-tools/kubeadm/kubeadm-init/)

![image-20220609104457209](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220609104457209.png)

pod是k8s的最小部署单元，一个机器有许多pod，pod与pod之间可能有联系，因此创建了一个pod的网络。

几个pod可以组成一个service，可以进行负载均衡操作，service与service之间也有通信，因此需要设置service网络。

![image-20220609113830781](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220609113830781.png)

2. **用身份运行**

```shell
To start using your cluster, you need to run the following as a regular user:
要开始使用集群，您需要以常规用户的身份运行以下命令：
mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config
```

3. **加入网络：**

可用的网络：[Installing Addons | Kubernetes](https://kubernetes.io/docs/concepts/cluster-administration/addons/)

使用![image-20220609111749887](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220609111749887.png)

安装Pod网络插件（CNI）

```shell
kubectl apply -f \   # 应用flannel网络
https://raw.githubusercontent.com/coreos/flanne/master/Documentation/kube-flannel.yml

kubectl delete -f .........  #删除flannel网络
```

如果 下载kube-flanne.yml比较慢，可以直接使用下载好的文件

```she
# 运行以下命令
kubectl apply -f kube-flannel.yml
```

![image-20220609112808667](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220609112808667.png)

网络运行完，暂时还不能使用

kube-flannel.yml

```yml
---
apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: psp.flannel.unprivileged
  annotations:
    seccomp.security.alpha.kubernetes.io/allowedProfileNames: docker/default
    seccomp.security.alpha.kubernetes.io/defaultProfileName: docker/default
    apparmor.security.beta.kubernetes.io/allowedProfileNames: runtime/default
    apparmor.security.beta.kubernetes.io/defaultProfileName: runtime/default
spec:
  privileged: false
  volumes:
    - configMap
    - secret
    - emptyDir
    - hostPath
  allowedHostPaths:
    - pathPrefix: "/etc/cni/net.d"
    - pathPrefix: "/etc/kube-flannel"
    - pathPrefix: "/run/flannel"
  readOnlyRootFilesystem: false
  # Users and groups
  runAsUser:
    rule: RunAsAny
  supplementalGroups:
    rule: RunAsAny
  fsGroup:
    rule: RunAsAny
  # Privilege Escalation
  allowPrivilegeEscalation: false
  defaultAllowPrivilegeEscalation: false
  # Capabilities
  allowedCapabilities: ['NET_ADMIN']
  defaultAddCapabilities: []
  requiredDropCapabilities: []
  # Host namespaces
  hostPID: false
  hostIPC: false
  hostNetwork: true
  hostPorts:
  - min: 0
    max: 65535
  # SELinux
  seLinux:
    # SELinux is unused in CaaSP
    rule: 'RunAsAny'
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: flannel
rules:
  - apiGroups: ['extensions']
    resources: ['podsecuritypolicies']
    verbs: ['use']
    resourceNames: ['psp.flannel.unprivileged']
  - apiGroups:
      - ""
    resources:
      - pods
    verbs:
      - get
  - apiGroups:
      - ""
    resources:
      - nodes
    verbs:
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - nodes/status
    verbs:
      - patch
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: flannel
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: flannel
subjects:
- kind: ServiceAccount
  name: flannel
  namespace: kube-system
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: flannel
  namespace: kube-system
---
kind: ConfigMap
apiVersion: v1
metadata:
  name: kube-flannel-cfg
  namespace: kube-system
  labels:
    tier: node
    app: flannel
data:
  cni-conf.json: |
    {
      "name": "cbr0",
      "cniVersion": "0.3.1",
      "plugins": [
        {
          "type": "flannel",
          "delegate": {
            "hairpinMode": true,
            "isDefaultGateway": true
          }
        },
        {
          "type": "portmap",
          "capabilities": {
            "portMappings": true
          }
        }
      ]
    }
  net-conf.json: |
    {
      "Network": "10.244.0.0/16",
      "Backend": {
        "Type": "vxlan"
      }
    }
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: kube-flannel-ds-amd64
  namespace: kube-system
  labels:
    tier: node
    app: flannel
spec:
  selector:
    matchLabels:
      app: flannel
  template:
    metadata:
      labels:
        tier: node
        app: flannel
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: beta.kubernetes.io/os
                    operator: In
                    values:
                      - linux
                  - key: beta.kubernetes.io/arch
                    operator: In
                    values:
                      - amd64
      hostNetwork: true
      tolerations:
      - operator: Exists
        effect: NoSchedule
      serviceAccountName: flannel
      initContainers:
      - name: install-cni
        image: quay.io/coreos/flannel:v0.11.0-amd64
        command:
        - cp
        args:
        - -f
        - /etc/kube-flannel/cni-conf.json
        - /etc/cni/net.d/10-flannel.conflist
        volumeMounts:
        - name: cni
          mountPath: /etc/cni/net.d
        - name: flannel-cfg
          mountPath: /etc/kube-flannel/
      containers:
      - name: kube-flannel
        image: quay.io/coreos/flannel:v0.11.0-amd64
        command:
        - /opt/bin/flanneld
        args:
        - --ip-masq
        - --kube-subnet-mgr
        resources:
          requests:
            cpu: "100m"
            memory: "50Mi"
          limits:
            cpu: "100m"
            memory: "50Mi"
        securityContext:
          privileged: false
          capabilities:
            add: ["NET_ADMIN"]
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        volumeMounts:
        - name: run
          mountPath: /run/flannel
        - name: flannel-cfg
          mountPath: /etc/kube-flannel/
      volumes:
        - name: run
          hostPath:
            path: /run/flannel
        - name: cni
          hostPath:
            path: /etc/cni/net.d
        - name: flannel-cfg
          configMap:
            name: kube-flannel-cfg
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: kube-flannel-ds-arm64
  namespace: kube-system
  labels:
    tier: node
    app: flannel
spec:
  selector:
    matchLabels:
      app: flannel
  template:
    metadata:
      labels:
        tier: node
        app: flannel
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: beta.kubernetes.io/os
                    operator: In
                    values:
                      - linux
                  - key: beta.kubernetes.io/arch
                    operator: In
                    values:
                      - arm64
      hostNetwork: true
      tolerations:
      - operator: Exists
        effect: NoSchedule
      serviceAccountName: flannel
      initContainers:
      - name: install-cni
        image: quay.io/coreos/flannel:v0.11.0-arm64
        command:
        - cp
        args:
        - -f
        - /etc/kube-flannel/cni-conf.json
        - /etc/cni/net.d/10-flannel.conflist
        volumeMounts:
        - name: cni
          mountPath: /etc/cni/net.d
        - name: flannel-cfg
          mountPath: /etc/kube-flannel/
      containers:
      - name: kube-flannel
        image: quay.io/coreos/flannel:v0.11.0-arm64
        command:
        - /opt/bin/flanneld
        args:
        - --ip-masq
        - --kube-subnet-mgr
        resources:
          requests:
            cpu: "100m"
            memory: "50Mi"
          limits:
            cpu: "100m"
            memory: "50Mi"
        securityContext:
          privileged: false
          capabilities:
             add: ["NET_ADMIN"]
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        volumeMounts:
        - name: run
          mountPath: /run/flannel
        - name: flannel-cfg
          mountPath: /etc/kube-flannel/
      volumes:
        - name: run
          hostPath:
            path: /run/flannel
        - name: cni
          hostPath:
            path: /etc/cni/net.d
        - name: flannel-cfg
          configMap:
            name: kube-flannel-cfg
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: kube-flannel-ds-arm
  namespace: kube-system
  labels:
    tier: node
    app: flannel
spec:
  selector:
    matchLabels:
      app: flannel
  template:
    metadata:
      labels:
        tier: node
        app: flannel
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: beta.kubernetes.io/os
                    operator: In
                    values:
                      - linux
                  - key: beta.kubernetes.io/arch
                    operator: In
                    values:
                      - arm
      hostNetwork: true
      tolerations:
      - operator: Exists
        effect: NoSchedule
      serviceAccountName: flannel
      initContainers:
      - name: install-cni
        image: quay.io/coreos/flannel:v0.11.0-arm
        command:
        - cp
        args:
        - -f
        - /etc/kube-flannel/cni-conf.json
        - /etc/cni/net.d/10-flannel.conflist
        volumeMounts:
        - name: cni
          mountPath: /etc/cni/net.d
        - name: flannel-cfg
          mountPath: /etc/kube-flannel/
      containers:
      - name: kube-flannel
        image: quay.io/coreos/flannel:v0.11.0-arm
        command:
        - /opt/bin/flanneld
        args:
        - --ip-masq
        - --kube-subnet-mgr
        resources:
          requests:
            cpu: "100m"
            memory: "50Mi"
          limits:
            cpu: "100m"
            memory: "50Mi"
        securityContext:
          privileged: false
          capabilities:
             add: ["NET_ADMIN"]
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        volumeMounts:
        - name: run
          mountPath: /run/flannel
        - name: flannel-cfg
          mountPath: /etc/kube-flannel/
      volumes:
        - name: run
          hostPath:
            path: /run/flannel
        - name: cni
          hostPath:
            path: /etc/cni/net.d
        - name: flannel-cfg
          configMap:
            name: kube-flannel-cfg
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: kube-flannel-ds-ppc64le
  namespace: kube-system
  labels:
    tier: node
    app: flannel
spec:
  selector:
    matchLabels:
      app: flannel
  template:
    metadata:
      labels:
        tier: node
        app: flannel
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: beta.kubernetes.io/os
                    operator: In
                    values:
                      - linux
                  - key: beta.kubernetes.io/arch
                    operator: In
                    values:
                      - ppc64le
      hostNetwork: true
      tolerations:
      - operator: Exists
        effect: NoSchedule
      serviceAccountName: flannel
      initContainers:
      - name: install-cni
        image: quay.io/coreos/flannel:v0.11.0-ppc64le
        command:
        - cp
        args:
        - -f
        - /etc/kube-flannel/cni-conf.json
        - /etc/cni/net.d/10-flannel.conflist
        volumeMounts:
        - name: cni
          mountPath: /etc/cni/net.d
        - name: flannel-cfg
          mountPath: /etc/kube-flannel/
      containers:
      - name: kube-flannel
        image: quay.io/coreos/flannel:v0.11.0-ppc64le
        command:
        - /opt/bin/flanneld
        args:
        - --ip-masq
        - --kube-subnet-mgr
        resources:
          requests:
            cpu: "100m"
            memory: "50Mi"
          limits:
            cpu: "100m"
            memory: "50Mi"
        securityContext:
          privileged: false
          capabilities:
             add: ["NET_ADMIN"]
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        volumeMounts:
        - name: run
          mountPath: /run/flannel
        - name: flannel-cfg
          mountPath: /etc/kube-flannel/
      volumes:
        - name: run
          hostPath:
            path: /run/flannel
        - name: cni
          hostPath:
            path: /etc/cni/net.d
        - name: flannel-cfg
          configMap:
            name: kube-flannel-cfg
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: kube-flannel-ds-s390x
  namespace: kube-system
  labels:
    tier: node
    app: flannel
spec:
  selector:
    matchLabels:
      app: flannel
  template:
    metadata:
      labels:
        tier: node
        app: flannel
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: beta.kubernetes.io/os
                    operator: In
                    values:
                      - linux
                  - key: beta.kubernetes.io/arch
                    operator: In
                    values:
                      - s390x
      hostNetwork: true
      tolerations:
      - operator: Exists
        effect: NoSchedule
      serviceAccountName: flannel
      initContainers:
      - name: install-cni
        image: quay.io/coreos/flannel:v0.11.0-s390x
        command:
        - cp
        args:
        - -f
        - /etc/kube-flannel/cni-conf.json
        - /etc/cni/net.d/10-flannel.conflist
        volumeMounts:
        - name: cni
          mountPath: /etc/cni/net.d
        - name: flannel-cfg
          mountPath: /etc/kube-flannel/
      containers:
      - name: kube-flannel
        image: quay.io/coreos/flannel:v0.11.0-s390x
        command:
        - /opt/bin/flanneld
        args:
        - --ip-masq
        - --kube-subnet-mgr
        resources:
          requests:
            cpu: "100m"
            memory: "50Mi"
          limits:
            cpu: "100m"
            memory: "50Mi"
        securityContext:
          privileged: false
          capabilities:
             add: ["NET_ADMIN"]
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        volumeMounts:
        - name: run
          mountPath: /run/flannel
        - name: flannel-cfg
          mountPath: /etc/kube-flannel/
      volumes:
        - name: run
          hostPath:
            path: /run/flannel
        - name: cni
          hostPath:
            path: /etc/cni/net.d
        - name: flannel-cfg
          configMap:
            name: kube-flannel-cfg
```

4. 获取pod

pod类似docker中的容器，pod有名称空间，获取pod时，需要指定获取某个空间的pod

```shell
[root@k8s-master k8s]# kubectl get pods
No resources found in default namespace.
```

查看名称空间：

```shell
kubectl get ns
```

![image-20220609113258023](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220609113258023.png)

查看所有名称空间的pods

```shell
kubectl get pods --all-namespaces
```

![image-20220609113342797](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220609113342797.png)

确定未running状态，再进行之后的操作。

5. 让其它所有节点加入master节点

```shell
# 查看所有节点
kubectl get nodes
```

![image-20220609113613449](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220609113613449.png)

6. 让其它节点加入master节点，执行以下命令

在k8s-node1和k8s-node2分别执行以下语句

```shell
kubeadm join 192.168.10.100:6443 --token 6tfp14.d1uy5xy2mjy0q113 \
    --discovery-token-ca-cert-hash sha256:f0d96f0381301e0fd7a41bb1c3d04898f6f7926be3d06578dda19d2879aad26e
```

**node1**

![image-20220609114119544](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220609114119544.png)

**master**

加入后，等1分钟左右状态会编程Ready

![image-20220609114200629](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220609114200629.png)

**监控各个节点状态：**

```shell
watch kubectl get pod -n kube-system -o wide
```

![image-20220609114402287](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220609114402287.png)

####  3. 创建容器

**以下操作在master操作**

创建一个tomcat pod

```shell
kubectl create deployment tomcat8 --image=tomcat:8.5.75-jre8
```

![image-20220609140602458](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220609140602458.png)

获取k8s中的所有资源

```shell
[root@k8s-master k8s]# kubectl get all
NAME                           READY   STATUS              RESTARTS   AGE
pod/tomcat8-84d78f8f54-g4jdg   0/1     ContainerCreating   0          69s   

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   178m

NAME                      READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/tomcat8   0/1     1            0           71s  # 部署了一个tomcat8，但是还没有成功

NAME                                 DESIRED   CURRENT   READY   AGE
replicaset.apps/tomcat8-84d78f8f54   1         1         0       71s
[root@k8s-master k8s]# 

```

查看更详细的资源信息

```shell
[root@k8s-master k8s]# kubectl get all -o wide
NAME                           READY   STATUS    RESTARTS   AGE     IP           NODE        NOMINATED NODE   READINESS GATES
pod/tomcat8-84d78f8f54-g4jdg   1/1     Running   0          3m37s   10.244.2.2   k8s-node2   <none>           <none>

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE   SELECTOR
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   3h    <none>

NAME                      READY   UP-TO-DATE   AVAILABLE   AGE     CONTAINERS   IMAGES               SELECTOR
deployment.apps/tomcat8   1/1     1            1           3m37s   tomcat       tomcat:8.5.75-jre8   app=tomcat8

NAME                                 DESIRED   CURRENT   READY   AGE     CONTAINERS   IMAGES               SELECTOR
replicaset.apps/tomcat8-84d78f8f54   1         1         1       3m37s   tomcat       tomcat:8.5.75-jre8   app=tomcat8,pod-template-hash=84d78f8f54

```

![image-20220609141004643](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220609141004643.png)

发现部署到了k8s-node2节点

查看node2节点

发现有tomcat镜像，并有一个正在运行的容器

![image-20220609141259250](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220609141259250.png)

在master上，查看pods信息

```shell
[root@k8s-master k8s]# kubectl get pods
NAME                       READY   STATUS    RESTARTS   AGE
tomcat8-84d78f8f54-g4jdg   1/1     Running   0          9m33s
```

查看所有的命名空间

```shell
kubectl get pods --all-namespaces
```

![image-20220609141831916](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220609141831916.png)

查看节点详情

![image-20220609141931339](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220609141931339.png)

我们发现，我们的tomcat服务被k8s指定部署到了node2节点上，如果服务宕机了，会怎么样呢？

#### 4. 节点宕机

1. **情况1：停止node2的tomcat容器**

![image-20220609142431585](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220609142431585.png)

2. **情况二：关闭node2服务**

此时只有主节点和node1节点存活

![image-20220609142653977](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220609142653977.png)

查看节点状态：

![image-20220609143847335](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220609143847335.png)

查看pod状态：

![image-20220609143516267](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220609143516267.png)

发现node2的节点状态已经是Teminating终端了，而node1中又新建了一个容器，正在创建中，体现了容灾恢复功能。

查看node1节点的容器

![image-20220609143702820](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220609143702820.png)

发现已经重新创建了一个tomcat容器

此时pod的状态：

![image-20220609143739061](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220609143739061.png)

重新启动node2

在查看pod状态：

发现node2的Teaminating的READY变成了1，说明停止指令已传送给了node2.（之前node2直接关闭，k8s发送给node2的关闭指令node2没有收到。。个人理解）

![image-20220609144312787](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220609144312787.png)

#### 5. 暴露容器端口

将tomcat容器的端口暴露给外界，可供外界访问

```shell
kubectl expose deployment tomcat8 --port=80 --target-port=8080 --type=NodePort  
# port端口是80，target-port端口是8080  --type的类型是NodePort，表示将port当作服务使用。会随机分配service的端口号，也可以指定端口
kubectl get svc 
# 查看service服务
```

```shell
[root@k8s-master ~]# kubectl expose deployment tomcat8 --port=80 --target-port=8080 --type=NodePort  
service/tomcat8 exposed
[root@k8s-master ~]# kubectl get svc
NAME         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE
kubernetes   ClusterIP   10.96.0.1      <none>        443/TCP        3h47m
tomcat8      NodePort    10.96.114.58   <none>        80:30254/TCP   9s
[root@k8s-master ~]# 
```

默认分配的service端口是30254

通过浏览器访问：

注意：如果访问404，需要把tomcat容器中webapps.dist下的文件移动到webapps文件夹中。

![image-20220609155020555](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220609155020555.png)



查看所有信息 kubectl get all

![image-20220609160101192](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220609160101192.png)

replicaset:复本控制器

#### 6. 动态扩容

将tomcat8的复本扩容3份

```shell
kubectl scale --replicas=3 deployment tomcat8
```

![image-20220609160831904](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220609160831904.png)

因为在node1和node2都有tomcat服务，因此在哪个节点访问30254端口都是可以的。

![image-20220609161035383](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20220609161035383.png)

![image-20220609161217494](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220609161217494.png)

![image-20220609161334553](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220609161334553.png)

![image-20220609161430866](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220609161430866.png)

缩容：

```shell
kubectl scale --replicas=1 deployment tomcat8
# 指定replicas的数量变小即可。
```

![image-20220609161916952](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220609161916952.png)

#### 7. 删除pod及service

指定名称即可删除

![image-20220609161831472](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220609161831472.png)

```shell
[root@k8s-master ~]# kubectl delete deployment.apps/tomcat8  # 删除部署信息，删除后对应的pod也会删除
deployment.apps "tomcat8" deleted
[root@k8s-master ~]# kubectl get all
NAME                 TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE
service/kubernetes   ClusterIP   10.96.0.1      <none>        443/TCP        5h11m
service/tomcat8      NodePort    10.96.114.58   <none>        80:30254/TCP   84m
[root@k8s-master ~]# kubectl delete service/tomcat8    # 删除service信息
service "tomcat8" deleted
[root@k8s-master ~]# kubectl get all
NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   5h12m
[root@k8s-master ~]# 

```

## 4. k8s细节

### 2. 使用yml创建服务

#### 1. 创建服务

1. 查看创建tomcatpod的yml文件

   1. ```shell
      kubectl create deployment tomcat8 --image=tomcat:8.5.72-jre8 --dry-run -o yaml
      ```

   2. ![image-20220609170024258](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220609170024258.png)

   3. --dry-run 表示模拟创建一个pod，并不会真的创建

2. 通过yml创建pod

   1. ```shell
      kubectl create deployment tomcat8 --image=tomcat:8.5.72-jre8 --dry-run -o yaml >tomcat8.yaml
      # 生成yaml信息
      ```

   2. ```shell
      [root@k8s-master k8s]# kubectl apply -f tomcat8.yaml
      deployment.apps/tomcat8 created
      [root@k8s-master k8s]# kubectl get nodes
      NAME         STATUS   ROLES    AGE     VERSION
      k8s-master   Ready    master   5h58m   v1.17.3
      k8s-node01   Ready    <none>   5h26m   v1.17.3
      k8s-node2    Ready    <none>   5h25m   v1.17.3
      [root@k8s-master k8s]# kubectl get pods
      NAME                       READY   STATUS              RESTARTS   AGE
      tomcat8-69879fd6b9-57bnc   0/1     ContainerCreating   0          26s
      [root@k8s-master k8s]# kubectl get pods -o wide
      NAME                       READY   STATUS              RESTARTS   AGE   IP       NODE        NOMINATED NODE   READINESS GATES
      tomcat8-69879fd6b9-57bnc   0/1     ContainerCreating   0          47s   <none>   k8s-node2   <none>           <none>
      [root@k8s-master k8s]# 
      
      ```

#### 2. 暴露服务

```shell
[root@k8s-master k8s]# kubectl expose deployment tomcat8 --port=80 --target-port=8080 --type=NodePort  --dry-run -o yaml
apiVersion: v1
kind: Service
metadata:
  creationTimestamp: null
  labels:
    app: tomcat8
  name: tomcat8
spec:
  ports:
  - port: 80
    protocol: TCP
    targetPort: 8080
  selector:
    app: tomcat8
  type: NodePort
status:
  loadBalancer: {}

```

#### 3. 查看某个pod

```shell
[root@k8s-master k8s]# kubectl get pod tomcat8-69879fd6b9-57bnc  -o yaml;
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: "2022-06-09T09:06:41Z"
  generateName: tomcat8-69879fd6b9-
  labels:
    app: tomcat8
    pod-template-hash: 69879fd6b9
  name: tomcat8-69879fd6b9-57bnc
  namespace: default
  ownerReferences:
  - apiVersion: apps/v1
    blockOwnerDeletion: true
    controller: true
    kind: ReplicaSet
    name: tomcat8-69879fd6b9
    uid: 20be7285-3731-407c-a790-4f2bd7e3d7eb
  resourceVersion: "51658"
  selfLink: /api/v1/namespaces/default/pods/tomcat8-69879fd6b9-57bnc
  uid: 29c2ee1d-a3e8-41ad-965d-69a7839922d5
spec:
  containers:
  - image: tomcat:8.5.72-jre8
    imagePullPolicy: IfNotPresent
    name: tomcat
    resources: {}
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: default-token-44dbh
      readOnly: true
  dnsPolicy: ClusterFirst
  enableServiceLinks: true
  nodeName: k8s-node2
  priority: 0
  restartPolicy: Always
  schedulerName: default-scheduler
  securityContext: {}
  serviceAccount: default
  serviceAccountName: default
  terminationGracePeriodSeconds: 30
  tolerations:
  - effect: NoExecute
    key: node.kubernetes.io/not-ready
    operator: Exists
    tolerationSeconds: 300
  - effect: NoExecute
    key: node.kubernetes.io/unreachable
    operator: Exists
    tolerationSeconds: 300
  volumes:
  - name: default-token-44dbh
    secret:
      defaultMode: 420
      secretName: default-token-44dbh
status:
  conditions:
  - lastProbeTime: null
    lastTransitionTime: "2022-06-09T09:06:41Z"
    status: "True"
    type: Initialized
  - lastProbeTime: null
    lastTransitionTime: "2022-06-09T09:08:48Z"
    status: "True"
    type: Ready
  - lastProbeTime: null
    lastTransitionTime: "2022-06-09T09:08:48Z"
    status: "True"
    type: ContainersReady
  - lastProbeTime: null
    lastTransitionTime: "2022-06-09T09:06:41Z"
    status: "True"
    type: PodScheduled
  containerStatuses:
  - containerID: docker://b98218fb5e726e4afd4a5d3e77d6c568e12e4ba8024cae6ec6e1f8d17bcc4975
    image: tomcat:8.5.72-jre8
    imageID: docker-pullable://tomcat@sha256:47165dabe7c092c61ef2726a7fdc70dda0ce9dea07ca68bda0aea6ecfaba2873
    lastState: {}
    name: tomcat
    ready: true
    restartCount: 0
    started: true
    state:
      running:
        startedAt: "2022-06-09T09:08:46Z"
  hostIP: 192.168.10.102
  phase: Running
  podIP: 10.244.2.5
  podIPs:
  - ip: 10.244.2.5
  qosClass: BestEffort
  startTime: "2022-06-09T09:06:41Z"

```

**创建一个pod**

```shell
[root@k8s-master k8s]# kubectl get pod tomcat8-69879fd6b9-57bnc  -o yaml;
apiVersion: v1
kind: Pod
metadata:
  labels:
    app: tomcat8-new
    pod-template-hash: 69879fd6b9
  name: tomcat8-new
  namespace: default
spec:
  containers:
  - image: tomcat:8.5.72-jre8
    imagePullPolicy: IfNotPresent  # 如果镜像不存在则拉取
    name:tomcattest
```

### 3. 概念

[Kubernetes Pod概述 _ Kubernetes(K8S)中文文档_Kubernetes中文社区](http://docs.kubernetes.org.cn/312.html)

### 4. Ingress

#### 使用yml创建tomcat pod

1. 生成tomcat8的yaml

   1. ```shell
      kubectl create deployment tomcat8 --image=tomcat:8.5.72-jre8--dry-run -o yaml > tomcat8-deployment
      ```

   2. ```shell
      apiVersion: apps/v1
      kind: Deployment
      metadata:
        labels:
          app: tomcat8
        name: tomcat8
      spec:
        replicas: 3
        selector:
          matchLabels:
            app: tomcat8
        template:
          metadata:
            creationTimestamp: null
            labels:
              app: tomcat8
          spec:
            containers:
            - image: tomcat:8.5.72-jre8--dry-run
              imagePullPolicy: IfNotPresent
              name: tomcat
      ```

2. 生成暴露端口的yaml

   1. ```shell
       kubectl expose deployment tomcat8 --port=80 --target-port=8080 --type=NodePort --dry-run -o yaml
      ```

   2. ```sh
      apiVersion: v1
      kind: Service
      metadata:
        creationTimestamp: null
        labels:
          app: tomcat8
        name: tomcat8
      spec:
        ports:
        - port: 80
          protocol: TCP
          targetPort: 8080
        selector:
          app: tomcat8
        type: NodePort
      status:
        loadBalancer: {}
      ```

3. 修改 tomcat8-deployment，让其包含暴露端口的yaml信息

   ```shell
   apiVersion: apps/v1
   kind: Deployment
   metadata:
     labels:
       app: tomcat8
     name: tomcat8
   spec:
     replicas: 3
     selector:
       matchLabels:
         app: tomcat8
     template:
       metadata:
         creationTimestamp: null
         labels:
           app: tomcat8
       spec:
         containers:
         - image: tomcat:8.5.72-jre8
           imagePullPolicy: IfNotPresent
           name: tomcat
   --- # 以三个-作为
   apiVersion: v1
   kind: Service
   metadata:
     creationTimestamp: null
     labels:
       app: tomcat8
     name: tomcat8
   spec:
     ports:
     - port: 80
       protocol: TCP
       targetPort: 8080
     selector:
       app: tomcat8
     type: NodePort
   ```


执行文件

```shell
kubectl apply -f tomcat8-deployment.yaml
```

![image-20220610092833262](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220610092833262.png)

成功：

![image-20220610092927996](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220610092927996.png)

#### 使用域名访问

使用Ingress进行域名访问，而非192.168.10.101这样执行服务去访问，如果某个结点宕机了，则可以负载均衡到其他服务。

通过ingress负载均衡到各个service

通过Ingress发现pod进行关联。基于域名访问
通过Ingress controller实现POD负载均衡
支持TCP/UDP 4层负载均衡和HTTP 7层负载均衡

![image-20220609191236130](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220609191236130.png)

每一个k8s节点里面，都会有一个ingress-conteoller

ingress是一个pod，里面是一个nginx容器

![image-20220609180357898](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220609180357898.png)

ingress底层使用nginx

![image-20220609175225583](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220609175225583.png)

对外暴漏的端口是80

![image-20220609180043579](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220609180043579.png)



![image-20220610093216141](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220610093216141.png)

**安装Ingress**

```shell
kubectl apply -f ingress-controller.yaml
```

![image-20220610093850374](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220610093850374.png)

查看所有节点：

多了两个ingresspod，用于子节点，master节点用于调度子节点

![image-20220610095734240](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220610095734240.png)

等待READY 1/1后，接着后续操作。

ingress-controller.yaml文件内容

```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx

---

kind: ConfigMap
apiVersion: v1
metadata:
  name: nginx-configuration
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx

---
kind: ConfigMap
apiVersion: v1
metadata:
  name: tcp-services
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx

---
kind: ConfigMap
apiVersion: v1
metadata:
  name: udp-services
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx

---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: nginx-ingress-serviceaccount
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx

---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  name: nginx-ingress-clusterrole
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
rules:
  - apiGroups:
      - ""
    resources:
      - configmaps
      - endpoints
      - nodes
      - pods
      - secrets
    verbs:
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - nodes
    verbs:
      - get
  - apiGroups:
      - ""
    resources:
      - services
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - "extensions"
    resources:
      - ingresses
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
      - patch
  - apiGroups:
      - "extensions"
    resources:
      - ingresses/status
    verbs:
      - update

---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: Role
metadata:
  name: nginx-ingress-role
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
rules:
  - apiGroups:
      - ""
    resources:
      - configmaps
      - pods
      - secrets
      - namespaces
    verbs:
      - get
  - apiGroups:
      - ""
    resources:
      - configmaps
    resourceNames:
      # Defaults to "<election-id>-<ingress-class>"
      # Here: "<ingress-controller-leader>-<nginx>"
      # This has to be adapted if you change either parameter
      # when launching the nginx-ingress-controller.
      - "ingress-controller-leader-nginx"
    verbs:
      - get
      - update
  - apiGroups:
      - ""
    resources:
      - configmaps
    verbs:
      - create
  - apiGroups:
      - ""
    resources:
      - endpoints
    verbs:
      - get

---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: RoleBinding
metadata:
  name: nginx-ingress-role-nisa-binding
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: nginx-ingress-role
subjects:
  - kind: ServiceAccount
    name: nginx-ingress-serviceaccount
    namespace: ingress-nginx

---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: nginx-ingress-clusterrole-nisa-binding
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: nginx-ingress-clusterrole
subjects:
  - kind: ServiceAccount
    name: nginx-ingress-serviceaccount
    namespace: ingress-nginx

---

apiVersion: apps/v1
kind: DaemonSet 
metadata:
  name: nginx-ingress-controller
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: ingress-nginx
      app.kubernetes.io/part-of: ingress-nginx
  template:
    metadata:
      labels:
        app.kubernetes.io/name: ingress-nginx
        app.kubernetes.io/part-of: ingress-nginx
      annotations:
        prometheus.io/port: "10254"
        prometheus.io/scrape: "true"
    spec:
      hostNetwork: true
      serviceAccountName: nginx-ingress-serviceaccount
      containers:
        - name: nginx-ingress-controller
          image: siriuszg/nginx-ingress-controller:0.20.0
          args:
            - /nginx-ingress-controller
            - --configmap=$(POD_NAMESPACE)/nginx-configuration
            - --tcp-services-configmap=$(POD_NAMESPACE)/tcp-services
            - --udp-services-configmap=$(POD_NAMESPACE)/udp-services
            - --publish-service=$(POD_NAMESPACE)/ingress-nginx
            - --annotations-prefix=nginx.ingress.kubernetes.io
          securityContext:
            allowPrivilegeEscalation: true
            capabilities:
              drop:
                - ALL
              add:
                - NET_BIND_SERVICE
            # www-data -> 33
            runAsUser: 33
          env:
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
          ports:
            - name: http
              containerPort: 80
            - name: https
              containerPort: 443
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 10254
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 10
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 10254
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 10

---
apiVersion: v1
kind: Service
metadata:
  name: ingress-nginx
  namespace: ingress-nginx
spec:
  #type: NodePort
  ports:
  - name: http
    port: 80
    targetPort: 80
    protocol: TCP
  - name: https
    port: 443
    targetPort: 443
    protocol: TCP
  selector:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx

```



**编写ingress配置**

```yaml
apiVersion: apps/v1
kind: Ingress 
metadata: 
  name: web 
spec:
  rules:
  - host: tomcat8.bigdata.com
    http:
      paths:
        - backend:
            serviceName: tomcat8 # 这里的serviceName与deployment.apps中的服务名相同。 如下图
            servicePort: 80  #我们之前配置时，是把tomcat 的 8080端口，暴露给pod的80端口，而pod的80端口，又对应着一个service端口，我们在此只需要配置一个pod的80端口即可。
```

![image-20220610095618269](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220610095618269.png)

执行命令：kubectl apply -f ingress-demo.yml

![image-20220610101221195](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220610101221195.png)

报错：

![image-20220610101310481](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220610101310481.png)

原因：

![image-20220610101402018](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220610101402018.png)

**修改本机host文件**

添加：![image-20220610101613966](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220610101613966.png)

成功：![image-20220610101706688](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220610101706688.png)

我在此配置的是1号节点的服务，停止1号服务再测试。

问题：

看讲解，理应是自动跳到node2节点上，能正常访问，但是目前已知访问不了，再等一会试试。

![image-20220610103938657](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220610103938657.png)

### 5. 图形化界面

#### dashboard

功能不强大

#### KubeSphere

参考文章：https://blog.csdn.net/CSDN877425287/article/details/108267932

**0.确认安装版本**

**3.0 教程**

注意：helm3.0之后已经不需要安装Tiller，但是本教程使用的是小于3.0的，后续可以改成3.0的，

![image-20220610151758731](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220610151758731.png)

3.0教程：https://v3-1.docs.kubesphere.io/zh/docs/quick-start/minimal-kubesphere-on-k8s/

安装时，大概需要二十分钟左右。

等所有pod安装完毕即可。

![image-20220610203858685](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220610203858685.png)

密码：KUBEsphere1457

完成：

![image-20220610203817256](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220610203817256.png)

相关的yml文件

1. kubesphere-installer.yaml

   ```shell
   ---
   apiVersion: apiextensions.k8s.io/v1beta1
   kind: CustomResourceDefinition
   metadata:
     name: clusterconfigurations.installer.kubesphere.io
   spec:
     group: installer.kubesphere.io
     versions:
     - name: v1alpha1
       served: true
       storage: true
     scope: Namespaced
     names:
       plural: clusterconfigurations
       singular: clusterconfiguration
       kind: ClusterConfiguration
       shortNames:
       - cc
   
   ---
   apiVersion: v1
   kind: Namespace
   metadata:
     name: kubesphere-system
   
   ---
   apiVersion: v1
   kind: ServiceAccount
   metadata:
     name: ks-installer
     namespace: kubesphere-system
   
   ---
   apiVersion: rbac.authorization.k8s.io/v1
   kind: ClusterRole
   metadata:
     name: ks-installer
   rules:
   - apiGroups:
     - ""
     resources:
     - '*'
     verbs:
     - '*'
   - apiGroups:
     - apps
     resources:
     - '*'
     verbs:
     - '*'
   - apiGroups:
     - extensions
     resources:
     - '*'
     verbs:
     - '*'
   - apiGroups:
     - batch
     resources:
     - '*'
     verbs:
     - '*'
   - apiGroups:
     - rbac.authorization.k8s.io
     resources:
     - '*'
     verbs:
     - '*'
   - apiGroups:
     - apiregistration.k8s.io
     resources:
     - '*'
     verbs:
     - '*'
   - apiGroups:
     - apiextensions.k8s.io
     resources:
     - '*'
     verbs:
     - '*'
   - apiGroups:
     - tenant.kubesphere.io
     resources:
     - '*'
     verbs:
     - '*'
   - apiGroups:
     - certificates.k8s.io
     resources:
     - '*'
     verbs:
     - '*'
   - apiGroups:
     - devops.kubesphere.io
     resources:
     - '*'
     verbs:
     - '*'
   - apiGroups:
     - monitoring.coreos.com
     resources:
     - '*'
     verbs:
     - '*'
   - apiGroups:
     - logging.kubesphere.io
     resources:
     - '*'
     verbs:
     - '*'
   - apiGroups:
     - jaegertracing.io
     resources:
     - '*'
     verbs:
     - '*'
   - apiGroups:
     - storage.k8s.io
     resources:
     - '*'
     verbs:
     - '*'
   - apiGroups:
     - admissionregistration.k8s.io
     resources:
     - '*'
     verbs:
     - '*'
   - apiGroups:
     - policy
     resources:
     - '*'
     verbs:
     - '*'
   - apiGroups:
     - autoscaling
     resources:
     - '*'
     verbs:
     - '*'
   - apiGroups:
     - networking.istio.io
     resources:
     - '*'
     verbs:
     - '*'
   - apiGroups:
     - config.istio.io
     resources:
     - '*'
     verbs:
     - '*'
   - apiGroups:
     - iam.kubesphere.io
     resources:
     - '*'
     verbs:
     - '*'
   - apiGroups:
     - notification.kubesphere.io
     resources:
     - '*'
     verbs:
     - '*'
   - apiGroups:
     - auditing.kubesphere.io
     resources:
     - '*'
     verbs:
     - '*'
   - apiGroups:
     - events.kubesphere.io
     resources:
     - '*'
     verbs:
     - '*'
   - apiGroups:
     - core.kubefed.io
     resources:
     - '*'
     verbs:
     - '*'
   - apiGroups:
     - installer.kubesphere.io
     resources:
     - '*'
     verbs:
     - '*'
   - apiGroups:
     - storage.kubesphere.io
     resources:
     - '*'
     verbs:
     - '*'
   - apiGroups:
     - security.istio.io
     resources:
     - '*'
     verbs:
     - '*'
   - apiGroups:
     - monitoring.kiali.io
     resources:
     - '*'
     verbs:
     - '*'
   - apiGroups:
     - kiali.io
     resources:
     - '*'
     verbs:
     - '*'
   - apiGroups:
     - networking.k8s.io
     resources:
     - '*'
     verbs:
     - '*'
   - apiGroups:
     - kubeedge.kubesphere.io
     resources:
     - '*'
     verbs:
     - '*'
   - apiGroups:
     - types.kubefed.io
     resources:
     - '*'
     verbs:
     - '*'
   
   ---
   kind: ClusterRoleBinding
   apiVersion: rbac.authorization.k8s.io/v1
   metadata:
     name: ks-installer
   subjects:
   - kind: ServiceAccount
     name: ks-installer
     namespace: kubesphere-system
   roleRef:
     kind: ClusterRole
     name: ks-installer
     apiGroup: rbac.authorization.k8s.io
   
   ---
   apiVersion: apps/v1
   kind: Deployment
   metadata:
     name: ks-installer
     namespace: kubesphere-system
     labels:
       app: ks-install
   spec:
     replicas: 1
     selector:
       matchLabels:
         app: ks-install
     template:
       metadata:
         labels:
           app: ks-install
       spec:
         serviceAccountName: ks-installer
         containers:
         - name: installer
           image: kubesphere/ks-installer:v3.1.1
           imagePullPolicy: "Always"
           resources:
             limits:
               cpu: "1"
               memory: 1Gi
             requests:
               cpu: 20m
               memory: 100Mi
           volumeMounts:
           - mountPath: /etc/localtime
             name: host-time
         volumes:
         - hostPath:
             path: /etc/localtime
             type: ""
           name: host-time
   
   ```

2. cluster-configuration.yaml

3. ```shell
   ---
   apiVersion: installer.kubesphere.io/v1alpha1
   kind: ClusterConfiguration
   metadata:
     name: ks-installer
     namespace: kubesphere-system
     labels:
       version: v3.1.1
   spec:
     persistence:
       storageClass: ""        # If there is no default StorageClass in your cluster, you need to specify an existing StorageClass here.
     authentication:
       jwtSecret: ""           # Keep the jwtSecret consistent with the Host Cluster. Retrieve the jwtSecret by executing "kubectl -n kubesphere-system get cm kubesphere-config -o yaml | grep -v "apiVersion" | grep jwtSecret" on the Host Cluster.
     local_registry: ""        # Add your private registry address if it is needed.
     etcd:
       monitoring: false       # Enable or disable etcd monitoring dashboard installation. You have to create a Secret for etcd before you enable it.
       endpointIps: localhost  # etcd cluster EndpointIps. It can be a bunch of IPs here.
       port: 2379              # etcd port.
       tlsEnable: true
     common:
       redis:
         enabled: false
       openldap:
         enabled: false
       minioVolumeSize: 20Gi # Minio PVC size.
       openldapVolumeSize: 2Gi   # openldap PVC size.
       redisVolumSize: 2Gi # Redis PVC size.
       monitoring:
         # type: external   # Whether to specify the external prometheus stack, and need to modify the endpoint at the next line.
         endpoint: http://prometheus-operated.kubesphere-monitoring-system.svc:9090 # Prometheus endpoint to get metrics data.
       es:   # Storage backend for logging, events and auditing.
         # elasticsearchMasterReplicas: 1   # The total number of master nodes. Even numbers are not allowed.
         # elasticsearchDataReplicas: 1     # The total number of data nodes.
         elasticsearchMasterVolumeSize: 4Gi   # The volume size of Elasticsearch master nodes.
         elasticsearchDataVolumeSize: 20Gi    # The volume size of Elasticsearch data nodes.
         logMaxAge: 7                     # Log retention time in built-in Elasticsearch. It is 7 days by default.
         elkPrefix: logstash              # The string making up index names. The index name will be formatted as ks-<elk_prefix>-log.
         basicAuth:
           enabled: false
           username: ""
           password: ""
         externalElasticsearchUrl: ""
         externalElasticsearchPort: ""
     console:
       enableMultiLogin: true  # Enable or disable simultaneous logins. It allows different users to log in with the same account at the same time.
       port: 30880
     alerting:                # (CPU: 0.1 Core, Memory: 100 MiB) It enables users to customize alerting policies to send messages to receivers in time with different time intervals and alerting levels to choose from.
       enabled: false         # Enable or disable the KubeSphere Alerting System.
       # thanosruler:
       #   replicas: 1
       #   resources: {}
     auditing:                # Provide a security-relevant chronological set of records，recording the sequence of activities happening on the platform, initiated by different tenants.
       enabled: false         # Enable or disable the KubeSphere Auditing Log System. 
     devops:                  # (CPU: 0.47 Core, Memory: 8.6 G) Provide an out-of-the-box CI/CD system based on Jenkins, and automated workflow tools including Source-to-Image & Binary-to-Image.
       enabled: false             # Enable or disable the KubeSphere DevOps System.
       jenkinsMemoryLim: 2Gi      # Jenkins memory limit.
       jenkinsMemoryReq: 1500Mi   # Jenkins memory request.
       jenkinsVolumeSize: 8Gi     # Jenkins volume size.
       jenkinsJavaOpts_Xms: 512m  # The following three fields are JVM parameters.
       jenkinsJavaOpts_Xmx: 512m
       jenkinsJavaOpts_MaxRAM: 2g
     events:                  # Provide a graphical web console for Kubernetes Events exporting, filtering and alerting in multi-tenant Kubernetes clusters.
       enabled: false         # Enable or disable the KubeSphere Events System.
       ruler:
         enabled: true
         replicas: 2
     logging:                 # (CPU: 57 m, Memory: 2.76 G) Flexible logging functions are provided for log query, collection and management in a unified console. Additional log collectors can be added, such as Elasticsearch, Kafka and Fluentd.
       enabled: false         # Enable or disable the KubeSphere Logging System.
       logsidecar:
         enabled: true
         replicas: 2
     metrics_server:                    # (CPU: 56 m, Memory: 44.35 MiB) It enables HPA (Horizontal Pod Autoscaler).
       enabled: false                   # Enable or disable metrics-server.
     monitoring:
       storageClass: ""                 # If there is an independent StorageClass you need for Prometheus, you can specify it here. The default StorageClass is used by default.
       # prometheusReplicas: 1          # Prometheus replicas are responsible for monitoring different segments of data source and providing high availability.
       prometheusMemoryRequest: 400Mi   # Prometheus request memory.
       prometheusVolumeSize: 20Gi       # Prometheus PVC size.
       # alertmanagerReplicas: 1          # AlertManager Replicas.
     multicluster:
       clusterRole: none  # host | member | none  # You can install a solo cluster, or specify it as the Host or Member Cluster.
     network:
       networkpolicy: # Network policies allow network isolation within the same cluster, which means firewalls can be set up between certain instances (Pods).
         # Make sure that the CNI network plugin used by the cluster supports NetworkPolicy. There are a number of CNI network plugins that support NetworkPolicy, including Calico, Cilium, Kube-router, Romana and Weave Net.
         enabled: false # Enable or disable network policies.
       ippool: # Use Pod IP Pools to manage the Pod network address space. Pods to be created can be assigned IP addresses from a Pod IP Pool.
         type: none # Specify "calico" for this field if Calico is used as your CNI plugin. "none" means that Pod IP Pools are disabled.
       topology: # Use Service Topology to view Service-to-Service communication based on Weave Scope.
         type: none # Specify "weave-scope" for this field to enable Service Topology. "none" means that Service Topology is disabled.
     openpitrix: # An App Store that is accessible to all platform tenants. You can use it to manage apps across their entire lifecycle.
       store:
         enabled: false # Enable or disable the KubeSphere App Store.
     servicemesh:         # (0.3 Core, 300 MiB) Provide fine-grained traffic management, observability and tracing, and visualized traffic topology.
       enabled: false     # Base component (pilot). Enable or disable KubeSphere Service Mesh (Istio-based).
     kubeedge:          # Add edge nodes to your cluster and deploy workloads on edge nodes.
       enabled: false   # Enable or disable KubeEdge.
       cloudCore:
         nodeSelector: {"node-role.kubernetes.io/worker": ""}
         tolerations: []
         cloudhubPort: "10000"
         cloudhubQuicPort: "10001"
         cloudhubHttpsPort: "10002"
         cloudstreamPort: "10003"
         tunnelPort: "10004"
         cloudHub:
           advertiseAddress: # At least a public IP address or an IP address which can be accessed by edge nodes must be provided.
             - ""            # Note that once KubeEdge is enabled, CloudCore will malfunction if the address is not provided.
           nodeLimit: "100"
         service:
           cloudhubNodePort: "30000"
           cloudhubQuicNodePort: "30001"
           cloudhubHttpsNodePort: "30002"
           cloudstreamNodePort: "30003"
           tunnelNodePort: "30004"
       edgeWatcher:
         nodeSelector: {"node-role.kubernetes.io/worker": ""}
         tolerations: []
         edgeWatcherAgent:
           nodeSelector: {"node-role.kubernetes.io/worker": ""}
           tolerations: []
   ```

4. 

**2.0教程**

没有实现

![image-20220610105015239](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220610105015239.png)

k8s版本

```shell
[root@k8s-master k8s]# kubectl version | grep Server
Server Version: version.Info{Major:"1", Minor:"17", GitVersion:"v1.17.3", GitCommit:"06ad960bfd03b39c8310aaf92d1e7c12ce618213", GitTreeState:"clean", BuildDate:"2020-02-11T18:07:13Z", GoVersion:"go1.13.6", Compiler:"gc", Platform:"linux/amd64"}
```

helm版本

![image-20220610105409078](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220610105409078.png)

##### **1. 安装helm（master节点）**

`helm` 是 `kubernetes` 的包管理器。 它相当于 `CentOS` 的 `yum` ，`Ubuntu` 的 `apt` 。

参考文章：[一定成功 k8s 安装helm v2.17 基本命令 - 春寒知冬冷 - 博客园 (cnblogs.com)](https://www.cnblogs.com/fanfanfanlichun/p/15513712.html)

1. 方式：将安装包和安装脚本都放在master的同一目录，之后执行脚本即可。

![image-20220610141957163](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220610141957163.png)

安装包：在电子书/k8s helm

![image-20220610141759044](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220610141759044.png)

安装脚本：

```shell
#!/usr/bin/env bash

# Copyright The Helm Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# The install script is based off of the MIT-licensed script from glide,
# the package manager for Go: https://github.com/Masterminds/glide.sh/blob/master/get

PROJECT_NAME="helm"
TILLER_NAME="tiller"

: ${USE_SUDO:="true"}
: ${HELM_INSTALL_DIR:="/usr/local/bin"}

# initArch discovers the architecture for this system.
initArch() {
  ARCH=$(uname -m)
  case $ARCH in
    armv5*) ARCH="armv5";;
    armv6*) ARCH="armv6";;
    armv7*) ARCH="arm";;
    aarch64) ARCH="arm64";;
    x86) ARCH="386";;
    x86_64) ARCH="amd64";;
    i686) ARCH="386";;
    i386) ARCH="386";;
  esac
}

# initOS discovers the operating system for this system.
initOS() {
  OS=$(echo `uname`|tr '[:upper:]' '[:lower:]')

  case "$OS" in
    # Minimalist GNU for Windows
    mingw*) OS='windows';;
  esac
}

# runs the given command as root (detects if we are root already)
runAsRoot() {
  if [ $EUID -ne 0 -a "$USE_SUDO" = "true" ]; then
    sudo "${@}"
  else
    "${@}"
  fi
}

# verifySupported checks that the os/arch combination is supported for
# binary builds.
verifySupported() {
  local supported="darwin-amd64\nlinux-386\nlinux-amd64\nlinux-arm\nlinux-arm64\nlinux-ppc64le\nlinux-s390x\nwindows-amd64"
  if ! echo "${supported}" | grep -q "${OS}-${ARCH}"; then
    echo "No prebuilt binary for ${OS}-${ARCH}."
    echo "To build from source, go to https://github.com/helm/helm"
    exit 1
  fi

  if ! type "curl" > /dev/null && ! type "wget" > /dev/null; then
    echo "Either curl or wget is required"
    exit 1
  fi
}

# checkDesiredVersion checks if the desired version is available.
checkDesiredVersion() {
  if [ "x$DESIRED_VERSION" == "x" ]; then
    # Pinning tag to v2.17.0 as per https://github.com/helm/helm/issues/9607
    TAG=v2.17.0
  else
    TAG=$DESIRED_VERSION
  fi
}

# checkHelmInstalledVersion checks which version of helm is installed and
# if it needs to be changed.
checkHelmInstalledVersion() {
  if [[ -f "${HELM_INSTALL_DIR}/${PROJECT_NAME}" ]]; then
    local version=$("${HELM_INSTALL_DIR}/${PROJECT_NAME}" version -c | grep '^Client' | cut -d'"' -f2)
    if [[ "$version" == "$TAG" ]]; then
      echo "Helm ${version} is already ${DESIRED_VERSION:-latest}"
      return 0
    else
      echo "Helm ${TAG} is available. Changing from version ${version}."
      return 1
    fi
  else
    return 1
  fi
}

# downloadFile downloads the latest binary package and also the checksum
# for that binary.
downloadFile() {
  HELM_DIST="helm-$TAG-$OS-$ARCH.tar.gz"
  DOWNLOAD_URL="https://get.helm.sh/$HELM_DIST"
  CHECKSUM_URL="$DOWNLOAD_URL.sha256"
  HELM_TMP_ROOT="$(mktemp -dt helm-installer-XXXXXX)"
  HELM_TMP_FILE="$HELM_TMP_ROOT/$HELM_DIST"
  HELM_SUM_FILE="$HELM_TMP_ROOT/$HELM_DIST.sha256"
  echo "Downloading $DOWNLOAD_URL"
  if type "curl" > /dev/null; then
    curl -SsL "$CHECKSUM_URL" -o "$HELM_SUM_FILE"
  elif type "wget" > /dev/null; then
    wget -q -O "$HELM_SUM_FILE" "$CHECKSUM_URL"
  fi
  if type "curl" > /dev/null; then
    #curl -SsL "$DOWNLOAD_URL" -o "$HELM_TMP_FILE"
    cp -rf ./helm-v2.17.0-linux-amd64.tar.gz $HELM_TMP_FILE    # 灏变慨鏀逛簡杩欎釜鍦版柟 璁╄剼鏈洿鎺ュ鍒剁殑鏈湴鍖呫€�
  elif type "wget" > /dev/null; then
    wget -q -O "$HELM_TMP_FILE" "$DOWNLOAD_URL"
  fi
}

# installFile verifies the SHA256 for the file, then unpacks and
# installs it.
installFile() {
  HELM_TMP="$HELM_TMP_ROOT/$PROJECT_NAME"
  local sum=$(openssl sha1 -sha256 ${HELM_TMP_FILE} | awk '{print $2}')
  local expected_sum=$(cat ${HELM_SUM_FILE})
  if [ "$sum" != "$expected_sum" ]; then
    echo "SHA sum of ${HELM_TMP_FILE} does not match. Aborting."
    exit 1
  fi

  mkdir -p "$HELM_TMP"
  tar xf "$HELM_TMP_FILE" -C "$HELM_TMP"
  HELM_TMP_BIN="$HELM_TMP/$OS-$ARCH/$PROJECT_NAME"
  TILLER_TMP_BIN="$HELM_TMP/$OS-$ARCH/$TILLER_NAME"
  echo "Preparing to install $PROJECT_NAME and $TILLER_NAME into ${HELM_INSTALL_DIR}"
  runAsRoot cp "$HELM_TMP_BIN" "$HELM_INSTALL_DIR"
  echo "$PROJECT_NAME installed into $HELM_INSTALL_DIR/$PROJECT_NAME"
  if [ -x "$TILLER_TMP_BIN" ]; then
    runAsRoot cp "$TILLER_TMP_BIN" "$HELM_INSTALL_DIR"
    echo "$TILLER_NAME installed into $HELM_INSTALL_DIR/$TILLER_NAME"
  else
    echo "info: $TILLER_NAME binary was not found in this release; skipping $TILLER_NAME installation"
  fi
}

# fail_trap is executed if an error occurs.
fail_trap() {
  result=$?
  if [ "$result" != "0" ]; then
    if [[ -n "$INPUT_ARGUMENTS" ]]; then
      echo "Failed to install $PROJECT_NAME with the arguments provided: $INPUT_ARGUMENTS"
      help
    else
      echo "Failed to install $PROJECT_NAME"
    fi
    echo -e "\tFor support, go to https://github.com/helm/helm."
  fi
  cleanup
  exit $result
}

# testVersion tests the installed client to make sure it is working.
testVersion() {
  set +e
  HELM="$(command -v $PROJECT_NAME)"
  if [ "$?" = "1" ]; then
    echo "$PROJECT_NAME not found. Is $HELM_INSTALL_DIR on your "'$PATH?'
    exit 1
  fi
  set -e
  echo "Run '$PROJECT_NAME init' to configure $PROJECT_NAME."
}

# help provides possible cli installation arguments
help () {
  echo "Accepted cli arguments are:"
  echo -e "\t[--help|-h ] ->> prints this help"
  echo -e "\t[--version|-v <desired_version>]"
  echo -e "\te.g. --version v2.4.0  or -v latest"
  echo -e "\t[--no-sudo]  ->> install without sudo"
}

# cleanup temporary files to avoid https://github.com/helm/helm/issues/2977
cleanup() {
  if [[ -d "${HELM_TMP_ROOT:-}" ]]; then
    rm -rf "$HELM_TMP_ROOT"
  fi
}

# Execution

#Stop execution on any error
trap "fail_trap" EXIT
set -e

# Parsing input arguments (if any)
export INPUT_ARGUMENTS="${@}"
set -u
while [[ $# -gt 0 ]]; do
  case $1 in
    '--version'|-v)
       shift
       if [[ $# -ne 0 ]]; then
           export DESIRED_VERSION="${1}"
       else
           echo -e "Please provide the desired version. e.g. --version v2.4.0 or -v latest"
           exit 0
       fi
       ;;
    '--no-sudo')
       USE_SUDO="false"
       ;;
    '--help'|-h)
       help
       exit 0
       ;;
    *) exit 1
       ;;
  esac
  shift
done
set +u

initArch
initOS
verifySupported
checkDesiredVersion
if ! checkHelmInstalledVersion; then
  downloadFile
  installFile
fi
testVersion
cleanup

```

##### 2. 安装tiller

1. 说白了就是接收到helm发送的请求，然后根据一系类 的配置文件生成k8s的部署文件（yaml文件），然后提交给k8s。

2. 执行  helm version

   ![image-20220610142242470](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220610142242470.png)

3. 执行 yum -y install socat

![image-20220610142152045](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220610142152045.png)

​		4. 创建tiller  rbac权限

1. 首先tiller想要把配置文件提交给k8s，那么它肯定需要权限才行。所以这一步就是给tiller权限。

2. 创建yaml  vi helm-rbac.yaml

   ```shell
   
   apiVersion: v1
   kind: ServiceAccount
   metadata:
     name: tiller
     namespace: kube-system
   ---
   apiVersion: rbac.authorization.k8s.io/v1
   kind: ClusterRoleBinding
   metadata:
     name: tiller
   roleRef:
     apiGroup: rbac.authorization.k8s.io
     kind: ClusterRole
     name: cluster-admin
   subjects:
     - kind: ServiceAccount
       name: tiller
       namespace: kube-system
   ```

3. 执行kubectl apply -f  helm-rbac.yaml

		4. 初始化helm
		
   		1. 其实就是在k8s起了一个tiller pod。

     ```shell
     helm init --upgrade -i registry.cn-hangzhou.aliyuncs.com/google_containers/tiller:v2.17.0 --stable-repo-url https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts --service-account=tiller --history-max 50
     ```

     2. tiller 的 pod

        ![image-20220610143215819](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220610143215819.png)

6. 分别执行helm 和 tiller   出现命令提示，则安装成功

7. 查看helm的安装版本

   ![image-20220610143515922](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220610143515922.png)

**卸载tiller**

遇到了helm与tiller版本不一致的问题，需要卸载tiller。

执行以下两条命令

```shell
kubectl get -n kube-system secrets,sa,clusterrolebinding -o name|grep tiller|xargs kubectl -n kube-system delete
kubectl get all -n kube-system -l app=helm -o name|xargs kubectl delete -n kube-system
```

##### 3.  安装OpenEBS

文档：https://v2-1.docs.kubesphere.io/docs/zh-CN/appendix/install-openebs/

![image-20220610143904505](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220610143904505.png)

![image-20220610144018966](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220610144018966.png)

1. 查看节点名称 kubectl get node -o wide

   ![image-20220610144110072](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220610144110072.png)

2. 确认 master 节点是否有 Taint，如下看到 master 节点有 Taint。

   1. kubectl describe node k8s-master | grep Taint

   ![image-20220610144153554](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220610144153554.png)

   2. 去掉master节点的Taint

      kubectl taint nodes k8s-master node-role.kubernetes.io/master:NoSchedule-

   ![image-20220610144525303](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220610144525303.png)

3. 创建 OpenEBS 的 namespace，OpenEBS 相关资源将创建在这个 namespace 下：

   kubectl create ns openebs

   ![image-20220610144701283](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220610144701283.png)

4. 安装openEBS

   1. 官方提供的方式

   2. ![image-20220610150419206](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220610150419206.png)

   3. 以上二者都不行   第一种失败，第二种链接失效。

   4. 解决方式：使用yaml方式安装

      1. 创建文件：vi openebs-operator.yaml

      2. ```shell
         #
         #                             DEPRECATION NOTICE
         #    This operator file is deprecated in 2.11.0 in favour of individual operators
         #       for each storage engine and the file will be removed in version 3.0.0
         #
         # Further specific components can be deploy using there individual operator yamls
         #
         # To deploy cStor:
         # https://github.com/openebs/charts/blob/gh-pages/cstor-operator.yaml
         #
         # To deploy Jiva:
         # https://github.com/openebs/charts/blob/gh-pages/jiva-operator.yaml
         #
         # To deploy Dynamic hostpath localpv provisioner:
         # https://github.com/openebs/charts/blob/gh-pages/hostpath-operator.yaml
         #
         #
         # This manifest deploys the OpenEBS control plane components, with associated CRs & RBAC rules
         # NOTE: On GKE, deploy the openebs-operator.yaml in admin context
         
         # Create the OpenEBS namespace
         apiVersion: v1
         kind: Namespace
         metadata:
           name: openebs
         ---
         # Create Maya Service Account
         apiVersion: v1
         kind: ServiceAccount
         metadata:
           name: openebs-maya-operator
           namespace: openebs
         ---
         # Define Role that allows operations on K8s pods/deployments
         kind: ClusterRole
         apiVersion: rbac.authorization.k8s.io/v1
         metadata:
           name: openebs-maya-operator
         rules:
         - apiGroups: ["*"]
           resources: ["nodes", "nodes/proxy"]
           verbs: ["*"]
         - apiGroups: ["*"]
           resources: ["namespaces", "services", "pods", "pods/exec", "deployments", "deployments/finalizers", "replicationcontrollers", "replicasets", "events", "endpoints", "configmaps", "secrets", "jobs", "cronjobs"]
           verbs: ["*"]
         - apiGroups: ["*"]
           resources: ["statefulsets", "daemonsets"]
           verbs: ["*"]
         - apiGroups: ["*"]
           resources: ["resourcequotas", "limitranges"]
           verbs: ["list", "watch"]
         - apiGroups: ["*"]
           resources: ["ingresses", "horizontalpodautoscalers", "verticalpodautoscalers", "certificatesigningrequests"]
           verbs: ["list", "watch"]
         - apiGroups: ["*"]
           resources: ["storageclasses", "persistentvolumeclaims", "persistentvolumes"]
           verbs: ["*"]
         - apiGroups: ["volumesnapshot.external-storage.k8s.io"]
           resources: ["volumesnapshots", "volumesnapshotdatas"]
           verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
         - apiGroups: ["apiextensions.k8s.io"]
           resources: ["customresourcedefinitions"]
           verbs: [ "get", "list", "create", "update", "delete", "patch"]
         - apiGroups: ["openebs.io"]
           resources: [ "*"]
           verbs: ["*" ]
         - apiGroups: ["cstor.openebs.io"]
           resources: [ "*"]
           verbs: ["*" ]
         - apiGroups: ["coordination.k8s.io"]
           resources: ["leases"]
           verbs: ["get", "watch", "list", "delete", "update", "create"]
         - apiGroups: ["admissionregistration.k8s.io"]
           resources: ["validatingwebhookconfigurations", "mutatingwebhookconfigurations"]
           verbs: ["get", "create", "list", "delete", "update", "patch"]
         - nonResourceURLs: ["/metrics"]
           verbs: ["get"]
         - apiGroups: ["*"]
           resources: ["poddisruptionbudgets"]
           verbs: ["get", "list", "create", "delete", "watch"]
         ---
         # Bind the Service Account with the Role Privileges.
         # TODO: Check if default account also needs to be there
         kind: ClusterRoleBinding
         apiVersion: rbac.authorization.k8s.io/v1
         metadata:
           name: openebs-maya-operator
         subjects:
         - kind: ServiceAccount
           name: openebs-maya-operator
           namespace: openebs
         roleRef:
           kind: ClusterRole
           name: openebs-maya-operator
           apiGroup: rbac.authorization.k8s.io
         ---
         apiVersion: apps/v1
         kind: Deployment
         metadata:
           name: maya-apiserver
           namespace: openebs
           labels:
             name: maya-apiserver
             openebs.io/component-name: maya-apiserver
             openebs.io/version: 2.12.0
         spec:
           selector:
             matchLabels:
               name: maya-apiserver
               openebs.io/component-name: maya-apiserver
           replicas: 1
           strategy:
             type: Recreate
             rollingUpdate: null
           template:
             metadata:
               labels:
                 name: maya-apiserver
                 openebs.io/component-name: maya-apiserver
                 openebs.io/version: 2.12.0
             spec:
               serviceAccountName: openebs-maya-operator
               containers:
               - name: maya-apiserver
                 imagePullPolicy: IfNotPresent
                 image: openebs/m-apiserver:2.12.0
                 ports:
                 - containerPort: 5656
                 env:
                 # OPENEBS_IO_KUBE_CONFIG enables maya api service to connect to K8s
                 # based on this config. This is ignored if empty.
                 # This is supported for maya api server version 0.5.2 onwards
                 #- name: OPENEBS_IO_KUBE_CONFIG
                 #  value: "/home/ubuntu/.kube/config"
                 # OPENEBS_IO_K8S_MASTER enables maya api service to connect to K8s
                 # based on this address. This is ignored if empty.
                 # This is supported for maya api server version 0.5.2 onwards
                 #- name: OPENEBS_IO_K8S_MASTER
                 #  value: "http://172.28.128.3:8080"
                 # OPENEBS_NAMESPACE provides the namespace of this deployment as an
                 # environment variable
                 - name: OPENEBS_NAMESPACE
                   valueFrom:
                     fieldRef:
                       fieldPath: metadata.namespace
                 # OPENEBS_SERVICE_ACCOUNT provides the service account of this pod as
                 # environment variable
                 - name: OPENEBS_SERVICE_ACCOUNT
                   valueFrom:
                     fieldRef:
                       fieldPath: spec.serviceAccountName
                 # OPENEBS_MAYA_POD_NAME provides the name of this pod as
                 # environment variable
                 - name: OPENEBS_MAYA_POD_NAME
                   valueFrom:
                     fieldRef:
                       fieldPath: metadata.name
                 # If OPENEBS_IO_CREATE_DEFAULT_STORAGE_CONFIG is false then OpenEBS default
                 # storageclass and storagepool will not be created.
                 - name: OPENEBS_IO_CREATE_DEFAULT_STORAGE_CONFIG
                   value: "true"
                 # OPENEBS_IO_INSTALL_DEFAULT_CSTOR_SPARSE_POOL decides whether default cstor sparse pool should be
                 # configured as a part of openebs installation.
                 # If "true" a default cstor sparse pool will be configured, if "false" it will not be configured.
                 # This value takes effect only if OPENEBS_IO_CREATE_DEFAULT_STORAGE_CONFIG
                 # is set to true
                 - name: OPENEBS_IO_INSTALL_DEFAULT_CSTOR_SPARSE_POOL
                   value: "false"
                 # OPENEBS_IO_INSTALL_CRD environment variable is used to enable/disable CRD installation
                 # from Maya API server. By default the CRDs will be installed
                 # - name: OPENEBS_IO_INSTALL_CRD
                 #   value: "true"
                 # OPENEBS_IO_BASE_DIR is used to configure base directory for openebs on host path.
                 # Where OpenEBS can store required files. Default base path will be /var/openebs
                 # - name: OPENEBS_IO_BASE_DIR
                 #   value: "/var/openebs"
                 # OPENEBS_IO_CSTOR_TARGET_DIR can be used to specify the hostpath
                 # to be used for saving the shared content between the side cars
                 # of cstor volume pod.
                 # The default path used is /var/openebs/sparse
                 #- name: OPENEBS_IO_CSTOR_TARGET_DIR
                 #  value: "/var/openebs/sparse"
                 # OPENEBS_IO_CSTOR_POOL_SPARSE_DIR can be used to specify the hostpath
                 # to be used for saving the shared content between the side cars
                 # of cstor pool pod. This ENV is also used to indicate the location
                 # of the sparse devices.
                 # The default path used is /var/openebs/sparse
                 #- name: OPENEBS_IO_CSTOR_POOL_SPARSE_DIR
                 #  value: "/var/openebs/sparse"
                 # OPENEBS_IO_JIVA_POOL_DIR can be used to specify the hostpath
                 # to be used for default Jiva StoragePool loaded by OpenEBS
                 # The default path used is /var/openebs
                 # This value takes effect only if OPENEBS_IO_CREATE_DEFAULT_STORAGE_CONFIG
                 # is set to true
                 #- name: OPENEBS_IO_JIVA_POOL_DIR
                 #  value: "/var/openebs"
                 # OPENEBS_IO_LOCALPV_HOSTPATH_DIR can be used to specify the hostpath
                 # to be used for default openebs-hostpath storageclass loaded by OpenEBS
                 # The default path used is /var/openebs/local
                 # This value takes effect only if OPENEBS_IO_CREATE_DEFAULT_STORAGE_CONFIG
                 # is set to true
                 #- name: OPENEBS_IO_LOCALPV_HOSTPATH_DIR
                 #  value: "/var/openebs/local"
                 - name: OPENEBS_IO_JIVA_CONTROLLER_IMAGE
                   value: "openebs/jiva:2.12.1"
                 - name: OPENEBS_IO_JIVA_REPLICA_IMAGE
                   value: "openebs/jiva:2.12.1"
                 - name: OPENEBS_IO_JIVA_REPLICA_COUNT
                   value: "3"
                 - name: OPENEBS_IO_CSTOR_TARGET_IMAGE
                   value: "openebs/cstor-istgt:2.12.0"
                 - name: OPENEBS_IO_CSTOR_POOL_IMAGE
                   value: "openebs/cstor-pool:2.12.0"
                 - name: OPENEBS_IO_CSTOR_POOL_MGMT_IMAGE
                   value: "openebs/cstor-pool-mgmt:2.12.0"
                 - name: OPENEBS_IO_CSTOR_VOLUME_MGMT_IMAGE
                   value: "openebs/cstor-volume-mgmt:2.12.0"
                 - name: OPENEBS_IO_VOLUME_MONITOR_IMAGE
                   value: "openebs/m-exporter:2.12.0"
                 - name: OPENEBS_IO_CSTOR_POOL_EXPORTER_IMAGE
                   value: "openebs/m-exporter:2.12.0"
                 - name: OPENEBS_IO_HELPER_IMAGE
                   value: "openebs/linux-utils:2.12.0"
                 # OPENEBS_IO_ENABLE_ANALYTICS if set to true sends anonymous usage
                 # events to Google Analytics
                 - name: OPENEBS_IO_ENABLE_ANALYTICS
                   value: "true"
                 - name: OPENEBS_IO_INSTALLER_TYPE
                   value: "openebs-operator"
                 # OPENEBS_IO_ANALYTICS_PING_INTERVAL can be used to specify the duration (in hours)
                 # for periodic ping events sent to Google Analytics.
                 # Default is 24h.
                 # Minimum is 1h. You can convert this to weekly by setting 168h
                 #- name: OPENEBS_IO_ANALYTICS_PING_INTERVAL
                 #  value: "24h"
                 livenessProbe:
                   exec:
                     command:
                     - sh
                     - -c
                     - /usr/local/bin/mayactl
                     - version
                   initialDelaySeconds: 30
                   periodSeconds: 60
                 readinessProbe:
                   exec:
                     command:
                     - sh
                     - -c
                     - /usr/local/bin/mayactl
                     - version
                   initialDelaySeconds: 30
                   periodSeconds: 60
         ---
         apiVersion: v1
         kind: Service
         metadata:
           name: maya-apiserver-service
           namespace: openebs
           labels:
             openebs.io/component-name: maya-apiserver-svc
         spec:
           ports:
           - name: api
             port: 5656
             protocol: TCP
             targetPort: 5656
           selector:
             name: maya-apiserver
           sessionAffinity: None
         ---
         apiVersion: apps/v1
         kind: Deployment
         metadata:
           name: openebs-provisioner
           namespace: openebs
           labels:
             name: openebs-provisioner
             openebs.io/component-name: openebs-provisioner
             openebs.io/version: 2.12.0
         spec:
           selector:
             matchLabels:
               name: openebs-provisioner
               openebs.io/component-name: openebs-provisioner
           replicas: 1
           strategy:
             type: Recreate
             rollingUpdate: null
           template:
             metadata:
               labels:
                 name: openebs-provisioner
                 openebs.io/component-name: openebs-provisioner
                 openebs.io/version: 2.12.0
             spec:
               serviceAccountName: openebs-maya-operator
               containers:
               - name: openebs-provisioner
                 imagePullPolicy: IfNotPresent
                 image: openebs/openebs-k8s-provisioner:2.12.0
                 env:
                 # OPENEBS_IO_K8S_MASTER enables openebs provisioner to connect to K8s
                 # based on this address. This is ignored if empty.
                 # This is supported for openebs provisioner version 0.5.2 onwards
                 #- name: OPENEBS_IO_K8S_MASTER
                 #  value: "http://10.128.0.12:8080"
                 # OPENEBS_IO_KUBE_CONFIG enables openebs provisioner to connect to K8s
                 # based on this config. This is ignored if empty.
                 # This is supported for openebs provisioner version 0.5.2 onwards
                 #- name: OPENEBS_IO_KUBE_CONFIG
                 #  value: "/home/ubuntu/.kube/config"
                 - name: NODE_NAME
                   valueFrom:
                     fieldRef:
                       fieldPath: spec.nodeName
                 - name: OPENEBS_NAMESPACE
                   valueFrom:
                     fieldRef:
                       fieldPath: metadata.namespace
                 # OPENEBS_MAYA_SERVICE_NAME provides the maya-apiserver K8s service name,
                 # that provisioner should forward the volume create/delete requests.
                 # If not present, "maya-apiserver-service" will be used for lookup.
                 # This is supported for openebs provisioner version 0.5.3-RC1 onwards
                 #- name: OPENEBS_MAYA_SERVICE_NAME
                 #  value: "maya-apiserver-apiservice"
                 # LEADER_ELECTION_ENABLED is used to enable/disable leader election. By default
                 # leader election is enabled.
                 #- name: LEADER_ELECTION_ENABLED
                 #  value: "true"
                 # OPENEBS_IO_JIVA_PATCH_NODE_AFFINITY is used to enable/disable setting node affinity
                 # to the jiva replica deployments. Default is `enabled`. The valid values are 
                 # `enabled` and `disabled`.
                 #- name: OPENEBS_IO_JIVA_PATCH_NODE_AFFINITY
                 #  value: "enabled"
                  # Process name used for matching is limited to the 15 characters
                  # present in the pgrep output.
                  # So fullname can't be used here with pgrep (>15 chars).A regular expression
                  # that matches the entire command name has to specified.
                  # Anchor `^` : matches any string that starts with `openebs-provis`
                  # `.*`: matches any string that has `openebs-provis` followed by zero or more char
                 livenessProbe:
                   exec:
                     command:
                     - sh
                     - -c
                     - test `pgrep -c "^openebs-provisi.*"` = 1
                   initialDelaySeconds: 30
                   periodSeconds: 60
         ---
         apiVersion: apps/v1
         kind: Deployment
         metadata:
           name: openebs-snapshot-operator
           namespace: openebs
           labels:
             name: openebs-snapshot-operator
             openebs.io/component-name: openebs-snapshot-operator
             openebs.io/version: 2.12.0
         spec:
           selector:
             matchLabels:
               name: openebs-snapshot-operator
               openebs.io/component-name: openebs-snapshot-operator
           replicas: 1
           strategy:
             type: Recreate
           template:
             metadata:
               labels:
                 name: openebs-snapshot-operator
                 openebs.io/component-name: openebs-snapshot-operator
                 openebs.io/version: 2.12.0
             spec:
               serviceAccountName: openebs-maya-operator
               containers:
                 - name: snapshot-controller
                   image: openebs/snapshot-controller:2.12.0
                   imagePullPolicy: IfNotPresent
                   env:
                   - name: OPENEBS_NAMESPACE
                     valueFrom:
                       fieldRef:
                         fieldPath: metadata.namespace
                  # Process name used for matching is limited to the 15 characters
                  # present in the pgrep output.
                  # So fullname can't be used here with pgrep (>15 chars).A regular expression
                  # that matches the entire command name has to specified.
                  # Anchor `^` : matches any string that starts with `snapshot-contro`
                  # `.*`: matches any string that has `snapshot-contro` followed by zero or more char
                   livenessProbe:
                     exec:
                       command:
                       - sh
                       - -c
                       - test `pgrep -c "^snapshot-contro.*"` = 1
                     initialDelaySeconds: 30
                     periodSeconds: 60
                 # OPENEBS_MAYA_SERVICE_NAME provides the maya-apiserver K8s service name,
                 # that snapshot controller should forward the snapshot create/delete requests.
                 # If not present, "maya-apiserver-service" will be used for lookup.
                 # This is supported for openebs provisioner version 0.5.3-RC1 onwards
                 #- name: OPENEBS_MAYA_SERVICE_NAME
                 #  value: "maya-apiserver-apiservice"
                 - name: snapshot-provisioner
                   image: openebs/snapshot-provisioner:2.12.0
                   imagePullPolicy: IfNotPresent
                   env:
                   - name: OPENEBS_NAMESPACE
                     valueFrom:
                       fieldRef:
                         fieldPath: metadata.namespace
                   # OPENEBS_MAYA_SERVICE_NAME provides the maya-apiserver K8s service name,
                   # that snapshot provisioner  should forward the clone create/delete requests.
                   # If not present, "maya-apiserver-service" will be used for lookup.
                   # This is supported for openebs provisioner version 0.5.3-RC1 onwards
                   #- name: OPENEBS_MAYA_SERVICE_NAME
                   #  value: "maya-apiserver-apiservice"
                   # LEADER_ELECTION_ENABLED is used to enable/disable leader election. By default
                   # leader election is enabled.
                   #- name: LEADER_ELECTION_ENABLED
                   #  value: "true"
                  # Process name used for matching is limited to the 15 characters
                  # present in the pgrep output.
                  # So fullname can't be used here with pgrep (>15 chars).A regular expression
                  # that matches the entire command name has to specified.
                  # Anchor `^` : matches any string that starts with `snapshot-provis`
                  # `.*`: matches any string that has `snapshot-provis` followed by zero or more char
                   livenessProbe:
                     exec:
                       command:
                       - sh
                       - -c
                       - test `pgrep -c "^snapshot-provis.*"` = 1
                     initialDelaySeconds: 30
                     periodSeconds: 60
         ---
         apiVersion: apiextensions.k8s.io/v1
         kind: CustomResourceDefinition
         metadata:
           annotations:
             controller-gen.kubebuilder.io/version: v0.5.0
           creationTimestamp: null
           name: blockdevices.openebs.io
         spec:
           group: openebs.io
           names:
             kind: BlockDevice
             listKind: BlockDeviceList
             plural: blockdevices
             shortNames:
               - bd
             singular: blockdevice
           scope: Namespaced
           versions:
             - additionalPrinterColumns:
                 - jsonPath: .spec.nodeAttributes.nodeName
                   name: NodeName
                   type: string
                 - jsonPath: .spec.path
                   name: Path
                   priority: 1
                   type: string
                 - jsonPath: .spec.filesystem.fsType
                   name: FSType
                   priority: 1
                   type: string
                 - jsonPath: .spec.capacity.storage
                   name: Size
                   type: string
                 - jsonPath: .status.claimState
                   name: ClaimState
                   type: string
                 - jsonPath: .status.state
                   name: Status
                   type: string
                 - jsonPath: .metadata.creationTimestamp
                   name: Age
                   type: date
               name: v1alpha1
               schema:
                 openAPIV3Schema:
                   description: BlockDevice is the Schema for the blockdevices API
                   properties:
                     apiVersion:
                       description: 'APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources'
                       type: string
                     kind:
                       description: 'Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds'
                       type: string
                     metadata:
                       type: object
                     spec:
                       description: DeviceSpec defines the properties and runtime status of a BlockDevice
                       properties:
                         aggregateDevice:
                           description: AggregateDevice was intended to store the hierarchical information in cases of LVM. However this is currently not implemented and may need to be re-looked into for better design. To be deprecated
                           type: string
                         capacity:
                           description: Capacity
                           properties:
                             logicalSectorSize:
                               description: LogicalSectorSize is blockdevice logical-sector size in bytes
                               format: int32
                               type: integer
                             physicalSectorSize:
                               description: PhysicalSectorSize is blockdevice physical-Sector size in bytes
                               format: int32
                               type: integer
                             storage:
                               description: Storage is the blockdevice capacity in bytes
                               format: int64
                               type: integer
                           required:
                             - storage
                           type: object
                         claimRef:
                           description: ClaimRef is the reference to the BDC which has claimed this BD
                           properties:
                             apiVersion:
                               description: API version of the referent.
                               type: string
                             fieldPath:
                               description: 'If referring to a piece of an object instead of an entire object, this string should contain a valid JSON/Go field access statement, such as desiredState.manifest.containers[2]. For example, if the object reference is to a container within a pod, this would take on a value like: "spec.containers{name}" (where "name" refers to the name of the container that triggered the event) or if no container name is specified "spec.containers[2]" (container with index 2 in this pod). This syntax is chosen only to have some well-defined way of referencing a part of an object. TODO: this design is not final and this field is subject to change in the future.'
                               type: string
                             kind:
                               description: 'Kind of the referent. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds'
                               type: string
                             name:
                               description: 'Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names'
                               type: string
                             namespace:
                               description: 'Namespace of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/'
                               type: string
                             resourceVersion:
                               description: 'Specific resourceVersion to which this reference is made, if any. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency'
                               type: string
                             uid:
                               description: 'UID of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#uids'
                               type: string
                           type: object
                         details:
                           description: Details contain static attributes of BD like model,serial, and so forth
                           properties:
                             compliance:
                               description: Compliance is standards/specifications version implemented by device firmware  such as SPC-1, SPC-2, etc
                               type: string
                             deviceType:
                               description: DeviceType represents the type of device like sparse, disk, partition, lvm, crypt
                               enum:
                                 - disk
                                 - partition
                                 - sparse
                                 - loop
                                 - lvm
                                 - crypt
                                 - dm
                                 - mpath
                               type: string
                             driveType:
                               description: DriveType is the type of backing drive, HDD/SSD
                               enum:
                                 - HDD
                                 - SSD
                                 - Unknown
                                 - ""
                               type: string
                             firmwareRevision:
                               description: FirmwareRevision is the disk firmware revision
                               type: string
                             hardwareSectorSize:
                               description: HardwareSectorSize is the hardware sector size in bytes
                               format: int32
                               type: integer
                             logicalBlockSize:
                               description: LogicalBlockSize is the logical block size in bytes reported by /sys/class/block/sda/queue/logical_block_size
                               format: int32
                               type: integer
                             model:
                               description: Model is model of disk
                               type: string
                             physicalBlockSize:
                               description: PhysicalBlockSize is the physical block size in bytes reported by /sys/class/block/sda/queue/physical_block_size
                               format: int32
                               type: integer
                             serial:
                               description: Serial is serial number of disk
                               type: string
                             vendor:
                               description: Vendor is vendor of disk
                               type: string
                           type: object
                         devlinks:
                           description: DevLinks contains soft links of a block device like /dev/by-id/... /dev/by-uuid/...
                           items:
                             description: DeviceDevLink holds the mapping between type and links like by-id type or by-path type link
                             properties:
                               kind:
                                 description: Kind is the type of link like by-id or by-path.
                                 enum:
                                   - by-id
                                   - by-path
                                 type: string
                               links:
                                 description: Links are the soft links
                                 items:
                                   type: string
                                 type: array
                             type: object
                           type: array
                         filesystem:
                           description: FileSystem contains mountpoint and filesystem type
                           properties:
                             fsType:
                               description: Type represents the FileSystem type of the block device
                               type: string
                             mountPoint:
                               description: MountPoint represents the mountpoint of the block device.
                               type: string
                           type: object
                         nodeAttributes:
                           description: NodeAttributes has the details of the node on which BD is attached
                           properties:
                             nodeName:
                               description: NodeName is the name of the Kubernetes node resource on which the device is attached
                               type: string
                           type: object
                         parentDevice:
                           description: "ParentDevice was intended to store the UUID of the parent Block Device as is the case for partitioned block devices. \n For example: /dev/sda is the parent for /dev/sda1 To be deprecated"
                           type: string
                         partitioned:
                           description: Partitioned represents if BlockDevice has partitions or not (Yes/No) Currently always default to No. To be deprecated
                           enum:
                             - "Yes"
                             - "No"
                           type: string
                         path:
                           description: Path contain devpath (e.g. /dev/sdb)
                           type: string
                       required:
                         - capacity
                         - devlinks
                         - nodeAttributes
                         - path
                       type: object
                     status:
                       description: DeviceStatus defines the observed state of BlockDevice
                       properties:
                         claimState:
                           description: ClaimState represents the claim state of the block device
                           enum:
                             - Claimed
                             - Unclaimed
                             - Released
                           type: string
                         state:
                           description: State is the current state of the blockdevice (Active/Inactive/Unknown)
                           enum:
                             - Active
                             - Inactive
                             - Unknown
                           type: string
                       required:
                         - claimState
                         - state
                       type: object
                   type: object
               served: true
               storage: true
               subresources: {}
         status:
           acceptedNames:
             kind: ""
             plural: ""
           conditions: []
           storedVersions: []
         
         ---
         apiVersion: apiextensions.k8s.io/v1
         kind: CustomResourceDefinition
         metadata:
           annotations:
             controller-gen.kubebuilder.io/version: v0.5.0
           creationTimestamp: null
           name: blockdeviceclaims.openebs.io
         spec:
           group: openebs.io
           names:
             kind: BlockDeviceClaim
             listKind: BlockDeviceClaimList
             plural: blockdeviceclaims
             shortNames:
               - bdc
             singular: blockdeviceclaim
           scope: Namespaced
           versions:
             - additionalPrinterColumns:
                 - jsonPath: .spec.blockDeviceName
                   name: BlockDeviceName
                   type: string
                 - jsonPath: .status.phase
                   name: Phase
                   type: string
                 - jsonPath: .metadata.creationTimestamp
                   name: Age
                   type: date
               name: v1alpha1
               schema:
                 openAPIV3Schema:
                   description: BlockDeviceClaim is the Schema for the blockdeviceclaims API
                   properties:
                     apiVersion:
                       description: 'APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources'
                       type: string
                     kind:
                       description: 'Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds'
                       type: string
                     metadata:
                       type: object
                     spec:
                       description: DeviceClaimSpec defines the request details for a BlockDevice
                       properties:
                         blockDeviceName:
                           description: BlockDeviceName is the reference to the block-device backing this claim
                           type: string
                         blockDeviceNodeAttributes:
                           description: BlockDeviceNodeAttributes is the attributes on the node from which a BD should be selected for this claim. It can include nodename, failure domain etc.
                           properties:
                             hostName:
                               description: HostName represents the hostname of the Kubernetes node resource where the BD should be present
                               type: string
                             nodeName:
                               description: NodeName represents the name of the Kubernetes node resource where the BD should be present
                               type: string
                           type: object
                         deviceClaimDetails:
                           description: Details of the device to be claimed
                           properties:
                             allowPartition:
                               description: AllowPartition represents whether to claim a full block device or a device that is a partition
                               type: boolean
                             blockVolumeMode:
                               description: 'BlockVolumeMode represents whether to claim a device in Block mode or Filesystem mode. These are use cases of BlockVolumeMode: 1) Not specified: VolumeMode check will not be effective 2) VolumeModeBlock: BD should not have any filesystem or mountpoint 3) VolumeModeFileSystem: BD should have a filesystem and mountpoint. If DeviceFormat is    specified then the format should match with the FSType in BD'
                               type: string
                             formatType:
                               description: Format of the device required, eg:ext4, xfs
                               type: string
                           type: object
                         deviceType:
                           description: DeviceType represents the type of drive like SSD, HDD etc.,
                           nullable: true
                           type: string
                         hostName:
                           description: Node name from where blockdevice has to be claimed. To be deprecated. Use NodeAttributes.HostName instead
                           type: string
                         resources:
                           description: Resources will help with placing claims on Capacity, IOPS
                           properties:
                             requests:
                               additionalProperties:
                                 anyOf:
                                   - type: integer
                                   - type: string
                                 pattern: ^(\+|-)?(([0-9]+(\.[0-9]*)?)|(\.[0-9]+))(([KMGTPE]i)|[numkMGTPE]|([eE](\+|-)?(([0-9]+(\.[0-9]*)?)|(\.[0-9]+))))?$
                                 x-kubernetes-int-or-string: true
                               description: 'Requests describes the minimum resources required. eg: if storage resource of 10G is requested minimum capacity of 10G should be available TODO for validating'
                               type: object
                           required:
                             - requests
                           type: object
                         selector:
                           description: Selector is used to find block devices to be considered for claiming
                           properties:
                             matchExpressions:
                               description: matchExpressions is a list of label selector requirements. The requirements are ANDed.
                               items:
                                 description: A label selector requirement is a selector that contains values, a key, and an operator that relates the key and values.
                                 properties:
                                   key:
                                     description: key is the label key that the selector applies to.
                                     type: string
                                   operator:
                                     description: operator represents a key's relationship to a set of values. Valid operators are In, NotIn, Exists and DoesNotExist.
                                     type: string
                                   values:
                                     description: values is an array of string values. If the operator is In or NotIn, the values array must be non-empty. If the operator is Exists or DoesNotExist, the values array must be empty. This array is replaced during a strategic merge patch.
                                     items:
                                       type: string
                                     type: array
                                 required:
                                   - key
                                   - operator
                                 type: object
                               type: array
                             matchLabels:
                               additionalProperties:
                                 type: string
                               description: matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels map is equivalent to an element of matchExpressions, whose key field is "key", the operator is "In", and the values array contains only "value". The requirements are ANDed.
                               type: object
                           type: object
                       type: object
                     status:
                       description: DeviceClaimStatus defines the observed state of BlockDeviceClaim
                       properties:
                         phase:
                           description: Phase represents the current phase of the claim
                           type: string
                       required:
                         - phase
                       type: object
                   type: object
               served: true
               storage: true
               subresources: {}
         status:
           acceptedNames:
             kind: ""
             plural: ""
           conditions: []
           storedVersions: []
         ---
         # This is the node-disk-manager related config.
         # It can be used to customize the disks probes and filters
         apiVersion: v1
         kind: ConfigMap
         metadata:
           name: openebs-ndm-config
           namespace: openebs
           labels:
             openebs.io/component-name: ndm-config
         data:
           # udev-probe is default or primary probe it should be enabled to run ndm
           # filterconfigs contains configs of filters. To provide a group of include
           # and exclude values add it as , separated string
           node-disk-manager.config: |
             probeconfigs:
               - key: udev-probe
                 name: udev probe
                 state: true
               - key: seachest-probe
                 name: seachest probe
                 state: false
               - key: smart-probe
                 name: smart probe
                 state: true
             filterconfigs:
               - key: os-disk-exclude-filter
                 name: os disk exclude filter
                 state: true
                 exclude: "/,/etc/hosts,/boot"
               - key: vendor-filter
                 name: vendor filter
                 state: true
                 include: ""
                 exclude: "CLOUDBYT,OpenEBS"
               - key: path-filter
                 name: path filter
                 state: true
                 include: ""
                 exclude: "/dev/loop,/dev/fd0,/dev/sr0,/dev/ram,/dev/md,/dev/dm-,/dev/rbd,/dev/zd"
         ---
         apiVersion: apps/v1
         kind: DaemonSet
         metadata:
           name: openebs-ndm
           namespace: openebs
           labels:
             name: openebs-ndm
             openebs.io/component-name: ndm
             openebs.io/version: 2.12.0
         spec:
           selector:
             matchLabels:
               name: openebs-ndm
               openebs.io/component-name: ndm
           updateStrategy:
             type: RollingUpdate
           template:
             metadata:
               labels:
                 name: openebs-ndm
                 openebs.io/component-name: ndm
                 openebs.io/version: 2.12.0
             spec:
               # By default the node-disk-manager will be run on all kubernetes nodes
               # If you would like to limit this to only some nodes, say the nodes
               # that have storage attached, you could label those node and use
               # nodeSelector.
               #
               # e.g. label the storage nodes with - "openebs.io/nodegroup"="storage-node"
               # kubectl label node <node-name> "openebs.io/nodegroup"="storage-node"
               #nodeSelector:
               #  "openebs.io/nodegroup": "storage-node"
               serviceAccountName: openebs-maya-operator
               hostNetwork: true
               # host PID is used to check status of iSCSI Service when the NDM
               # API service is enabled
               #hostPID: true
               containers:
               - name: node-disk-manager
                 image: openebs/node-disk-manager:1.6.1
                 args:
                   - -v=4
                   # The feature-gate is used to enable the new UUID algorithm.
                   - --feature-gates="GPTBasedUUID"
                 # Detect mount point and filesystem changes wihtout restart.
                 # Uncomment the line below to enable the feature.
                 # --feature-gates="MountChangeDetection"
                 # The feature gate is used to start the gRPC API service. The gRPC server
                 # starts at 9115 port by default. This feature is currently in Alpha state
                 # - --feature-gates="APIService"
                 # The feature gate is used to enable NDM, to create blockdevice resources
                 # for unused partitions on the OS disk
                 # - --feature-gates="UseOSDisk"
                 imagePullPolicy: IfNotPresent
                 securityContext:
                   privileged: true
                 volumeMounts:
                 - name: config
                   mountPath: /host/node-disk-manager.config
                   subPath: node-disk-manager.config
                   readOnly: true
                   # make udev database available inside container
                 - name: udev
                   mountPath: /run/udev
                 - name: procmount
                   mountPath: /host/proc
                   readOnly: true
                 - name: devmount
                   mountPath: /dev
                 - name: basepath
                   mountPath: /var/openebs/ndm
                 - name: sparsepath
                   mountPath: /var/openebs/sparse
                 env:
                 # namespace in which NDM is installed will be passed to NDM Daemonset
                 # as environment variable
                 - name: NAMESPACE
                   valueFrom:
                     fieldRef:
                       fieldPath: metadata.namespace
                 # pass hostname as env variable using downward API to the NDM container
                 - name: NODE_NAME
                   valueFrom:
                     fieldRef:
                       fieldPath: spec.nodeName
                 # specify the directory where the sparse files need to be created.
                 # if not specified, then sparse files will not be created.
                 - name: SPARSE_FILE_DIR
                   value: "/var/openebs/sparse"
                 # Size(bytes) of the sparse file to be created.
                 - name: SPARSE_FILE_SIZE
                   value: "10737418240"
                 # Specify the number of sparse files to be created
                 - name: SPARSE_FILE_COUNT
                   value: "0"
                 livenessProbe:
                   exec:
                     command:
                     - pgrep
                     - "ndm"
                   initialDelaySeconds: 30
                   periodSeconds: 60
               volumes:
               - name: config
                 configMap:
                   name: openebs-ndm-config
               - name: udev
                 hostPath:
                   path: /run/udev
                   type: Directory
               # mount /proc (to access mount file of process 1 of host) inside container
               # to read mount-point of disks and partitions
               - name: procmount
                 hostPath:
                   path: /proc
                   type: Directory
               - name: devmount
               # the /dev directory is mounted so that we have access to the devices that
               # are connected at runtime of the pod.
                 hostPath:
                   path: /dev
                   type: Directory
               - name: basepath
                 hostPath:
                   path: /var/openebs/ndm
                   type: DirectoryOrCreate
               - name: sparsepath
                 hostPath:
                   path: /var/openebs/sparse
         ---
         apiVersion: apps/v1
         kind: Deployment
         metadata:
           name: openebs-ndm-operator
           namespace: openebs
           labels:
             name: openebs-ndm-operator
             openebs.io/component-name: ndm-operator
             openebs.io/version: 2.12.0
         spec:
           selector:
             matchLabels:
               name: openebs-ndm-operator
               openebs.io/component-name: ndm-operator
           replicas: 1
           strategy:
             type: Recreate
           template:
             metadata:
               labels:
                 name: openebs-ndm-operator
                 openebs.io/component-name: ndm-operator
                 openebs.io/version: 2.12.0
             spec:
               serviceAccountName: openebs-maya-operator
               containers:
                 - name: node-disk-operator
                   image: openebs/node-disk-operator:1.6.1
                   imagePullPolicy: IfNotPresent
                   env:
                     - name: WATCH_NAMESPACE
                       valueFrom:
                         fieldRef:
                           fieldPath: metadata.namespace
                     - name: POD_NAME
                       valueFrom:
                         fieldRef:
                           fieldPath: metadata.name
                     # the service account of the ndm-operator pod
                     - name: SERVICE_ACCOUNT
                       valueFrom:
                         fieldRef:
                           fieldPath: spec.serviceAccountName
                     - name: OPERATOR_NAME
                       value: "node-disk-operator"
                     - name: CLEANUP_JOB_IMAGE
                       value: "openebs/linux-utils:2.12.0"
                     # OPENEBS_IO_IMAGE_PULL_SECRETS environment variable is used to pass the image pull secrets
                     # to the cleanup pod launched by NDM operator
                     #- name: OPENEBS_IO_IMAGE_PULL_SECRETS
                     #  value: ""
                   livenessProbe:
                     httpGet:
                       path: /healthz
                       port: 8585
                     initialDelaySeconds: 15
                     periodSeconds: 20
                   readinessProbe:
                     httpGet:
                       path: /readyz
                       port: 8585
                     initialDelaySeconds: 5
                     periodSeconds: 10
         ---
         apiVersion: apps/v1
         kind: Deployment
         metadata:
           name: openebs-admission-server
           namespace: openebs
           labels:
             app: admission-webhook
             openebs.io/component-name: admission-webhook
             openebs.io/version: 2.12.0
         spec:
           replicas: 1
           strategy:
             type: Recreate
             rollingUpdate: null
           selector:
             matchLabels:
               app: admission-webhook
           template:
             metadata:
               labels:
                 app: admission-webhook
                 openebs.io/component-name: admission-webhook
                 openebs.io/version: 2.12.0
             spec:
               serviceAccountName: openebs-maya-operator
               containers:
                 - name: admission-webhook
                   image: openebs/admission-server:2.12.0
                   imagePullPolicy: IfNotPresent
                   args:
                     - -alsologtostderr
                     - -v=2
                     - 2>&1
                   env:
                     - name: OPENEBS_NAMESPACE
                       valueFrom:
                         fieldRef:
                           fieldPath: metadata.namespace
                     - name: ADMISSION_WEBHOOK_NAME
                       value: "openebs-admission-server"
                     - name: ADMISSION_WEBHOOK_FAILURE_POLICY
                       value: "Fail"
                   # Process name used for matching is limited to the 15 characters
                   # present in the pgrep output.
                   # So fullname can't be used here with pgrep (>15 chars).A regular expression
                   # Anchor `^` : matches any string that starts with `admission-serve`
                   # `.*`: matche any string that has `admission-serve` followed by zero or more char
                   # that matches the entire command name has to specified.
                   livenessProbe:
                     exec:
                       command:
                       - sh
                       - -c
                       - test `pgrep -c "^admission-serve.*"` = 1
                     initialDelaySeconds: 30
                     periodSeconds: 60
         ---
         apiVersion: apps/v1
         kind: Deployment
         metadata:
           name: openebs-localpv-provisioner
           namespace: openebs
           labels:
             name: openebs-localpv-provisioner
             openebs.io/component-name: openebs-localpv-provisioner
             openebs.io/version: 2.12.0
         spec:
           selector:
             matchLabels:
               name: openebs-localpv-provisioner
               openebs.io/component-name: openebs-localpv-provisioner
           replicas: 1
           strategy:
             type: Recreate
           template:
             metadata:
               labels:
                 name: openebs-localpv-provisioner
                 openebs.io/component-name: openebs-localpv-provisioner
                 openebs.io/version: 2.12.0
             spec:
               serviceAccountName: openebs-maya-operator
               containers:
               - name: openebs-provisioner-hostpath
                 imagePullPolicy: IfNotPresent
                 image: openebs/provisioner-localpv:2.12.0
                 args:
                   - "--bd-time-out=$(BDC_BD_BIND_RETRIES)"
                 env:
                 # OPENEBS_IO_K8S_MASTER enables openebs provisioner to connect to K8s
                 # based on this address. This is ignored if empty.
                 # This is supported for openebs provisioner version 0.5.2 onwards
                 #- name: OPENEBS_IO_K8S_MASTER
                 #  value: "http://10.128.0.12:8080"
                 # OPENEBS_IO_KUBE_CONFIG enables openebs provisioner to connect to K8s
                 # based on this config. This is ignored if empty.
                 # This is supported for openebs provisioner version 0.5.2 onwards
                 #- name: OPENEBS_IO_KUBE_CONFIG
                 #  value: "/home/ubuntu/.kube/config"
                 #  This sets the number of times the provisioner should try 
                 #  with a polling interval of 5 seconds, to get the Blockdevice
                 #  Name from a BlockDeviceClaim, before the BlockDeviceClaim
                 #  is deleted. E.g. 12 * 5 seconds = 60 seconds timeout
                 - name: BDC_BD_BIND_RETRIES
                   value: "12"
                 - name: NODE_NAME
                   valueFrom:
                     fieldRef:
                       fieldPath: spec.nodeName
                 - name: OPENEBS_NAMESPACE
                   valueFrom:
                     fieldRef:
                       fieldPath: metadata.namespace
                 # OPENEBS_SERVICE_ACCOUNT provides the service account of this pod as
                 # environment variable
                 - name: OPENEBS_SERVICE_ACCOUNT
                   valueFrom:
                     fieldRef:
                       fieldPath: spec.serviceAccountName
                 - name: OPENEBS_IO_ENABLE_ANALYTICS
                   value: "true"
                 - name: OPENEBS_IO_INSTALLER_TYPE
                   value: "openebs-operator"
                 - name: OPENEBS_IO_HELPER_IMAGE
                   value: "openebs/linux-utils:2.12.0"
                 - name: OPENEBS_IO_BASE_PATH
                   value: "/var/openebs/local"
                 # LEADER_ELECTION_ENABLED is used to enable/disable leader election. By default
                 # leader election is enabled.
                 #- name: LEADER_ELECTION_ENABLED
                 #  value: "true"
                 # OPENEBS_IO_IMAGE_PULL_SECRETS environment variable is used to pass the image pull secrets
                 # to the helper pod launched by local-pv hostpath provisioner
                 #- name: OPENEBS_IO_IMAGE_PULL_SECRETS
                 #  value: ""
                 # Process name used for matching is limited to the 15 characters
                 # present in the pgrep output.
                 # So fullname can't be used here with pgrep (>15 chars).A regular expression
                 # that matches the entire command name has to specified.
                 # Anchor `^` : matches any string that starts with `provisioner-loc`
                 # `.*`: matches any string that has `provisioner-loc` followed by zero or more char
                 livenessProbe:
                   exec:
                     command:
                     - sh
                     - -c
                     - test `pgrep -c "^provisioner-loc.*"` = 1
                   initialDelaySeconds: 30
                   periodSeconds: 60
         ---
         
         ```

      3. 应用新建的文件：kubectl apply -f openebs-operator.yaml

      4. 查看OpenEBS的创建情况

      5. ![image-20220610150749322](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220610150749322.png)

      6. 查看创建的StorageClass

         ![image-20220610150849148](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220610150849148.png)

         安装成功。

         7. 如下将 openebs-hostpath设置为 默认的 StorageClass：

            ```shell
            kubectl patch storageclass openebs-hostpath -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'
            ```

            ![image-20220610151055754](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220610151055754.png)

##### 4. 安装kbuesphere

使用最小化安装

![image-20220610154139536](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220610154139536.png)

![image-20220610154157178](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220610154157178.png)

安装失败，使用yml的方式安装

1. kubectl apply -f kubesphere-installer.yaml

2. kubectl apply -f cluster-configuration.yaml

3. 检查安装日志（查看安装过程）：kubectl logs -n kubesphere-system $(kubectl get pod -n kubesphere-system -l app=ks-install -o jsonpath='{.items[0].metadata.name}') -f

4. 用于安装kbuesphere的pod

   ![image-20220610155808573](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220610155808573.png)

1. vi kubesphere-installer.yaml

   ```shell
   ---
   apiVersion: apiextensions.k8s.io/v1beta1
   kind: CustomResourceDefinition
   metadata:
     name: clusterconfigurations.installer.kubesphere.io
   spec:
     group: installer.kubesphere.io
     versions:
     - name: v1alpha1
       served: true
       storage: true
     scope: Namespaced
     names:
       plural: clusterconfigurations
       singular: clusterconfiguration
       kind: ClusterConfiguration
       shortNames:
       - cc
   
   ---
   apiVersion: v1
   kind: Namespace
   metadata:
     name: kubesphere-system
   
   ---
   apiVersion: v1
   kind: ServiceAccount
   metadata:
     name: ks-installer
     namespace: kubesphere-system
   
   ---
   apiVersion: rbac.authorization.k8s.io/v1
   kind: ClusterRole
   metadata:
     name: ks-installer
   rules:
   - apiGroups:
     - ""
     resources:
     - '*'
     verbs:
     - '*'
   - apiGroups:
     - apps
     resources:
     - '*'
     verbs:
     - '*'
   - apiGroups:
     - extensions
     resources:
     - '*'
     verbs:
     - '*'
   - apiGroups:
     - batch
     resources:
     - '*'
     verbs:
     - '*'
   - apiGroups:
     - rbac.authorization.k8s.io
     resources:
     - '*'
     verbs:
     - '*'
   - apiGroups:
     - apiregistration.k8s.io
     resources:
     - '*'
     verbs:
     - '*'
   - apiGroups:
     - apiextensions.k8s.io
     resources:
     - '*'
     verbs:
     - '*'
   - apiGroups:
     - tenant.kubesphere.io
     resources:
     - '*'
     verbs:
     - '*'
   - apiGroups:
     - certificates.k8s.io
     resources:
     - '*'
     verbs:
     - '*'
   - apiGroups:
     - devops.kubesphere.io
     resources:
     - '*'
     verbs:
     - '*'
   - apiGroups:
     - monitoring.coreos.com
     resources:
     - '*'
     verbs:
     - '*'
   - apiGroups:
     - logging.kubesphere.io
     resources:
     - '*'
     verbs:
     - '*'
   - apiGroups:
     - jaegertracing.io
     resources:
     - '*'
     verbs:
     - '*'
   - apiGroups:
     - storage.k8s.io
     resources:
     - '*'
     verbs:
     - '*'
   - apiGroups:
     - admissionregistration.k8s.io
     resources:
     - '*'
     verbs:
     - '*'
   - apiGroups:
     - policy
     resources:
     - '*'
     verbs:
     - '*'
   - apiGroups:
     - autoscaling
     resources:
     - '*'
     verbs:
     - '*'
   - apiGroups:
     - networking.istio.io
     resources:
     - '*'
     verbs:
     - '*'
   - apiGroups:
     - config.istio.io
     resources:
     - '*'
     verbs:
     - '*'
   - apiGroups:
     - iam.kubesphere.io
     resources:
     - '*'
     verbs:
     - '*'
   - apiGroups:
     - notification.kubesphere.io
     resources:
     - '*'
     verbs:
     - '*'
   - apiGroups:
     - auditing.kubesphere.io
     resources:
     - '*'
     verbs:
     - '*'
   - apiGroups:
     - events.kubesphere.io
     resources:
     - '*'
     verbs:
     - '*'
   - apiGroups:
     - core.kubefed.io
     resources:
     - '*'
     verbs:
     - '*'
   - apiGroups:
     - installer.kubesphere.io
     resources:
     - '*'
     verbs:
     - '*'
   - apiGroups:
     - storage.kubesphere.io
     resources:
     - '*'
     verbs:
     - '*'
   - apiGroups:
     - security.istio.io
     resources:
     - '*'
     verbs:
     - '*'
   - apiGroups:
     - monitoring.kiali.io
     resources:
     - '*'
     verbs:
     - '*'
   - apiGroups:
     - kiali.io
     resources:
     - '*'
     verbs:
     - '*'
   - apiGroups:
     - networking.k8s.io
     resources:
     - '*'
     verbs:
     - '*'
   - apiGroups:
     - kubeedge.kubesphere.io
     resources:
     - '*'
     verbs:
     - '*'
   - apiGroups:
     - types.kubefed.io
     resources:
     - '*'
     verbs:
     - '*'
   
   ---
   kind: ClusterRoleBinding
   apiVersion: rbac.authorization.k8s.io/v1
   metadata:
     name: ks-installer
   subjects:
   - kind: ServiceAccount
     name: ks-installer
     namespace: kubesphere-system
   roleRef:
     kind: ClusterRole
     name: ks-installer
     apiGroup: rbac.authorization.k8s.io
   
   ---
   apiVersion: apps/v1
   kind: Deployment
   metadata:
     name: ks-installer
     namespace: kubesphere-system
     labels:
       app: ks-install
   spec:
     replicas: 1
     selector:
       matchLabels:
         app: ks-install
     template:
       metadata:
         labels:
           app: ks-install
       spec:
         serviceAccountName: ks-installer
         containers:
         - name: installer
           image: kubesphere/ks-installer:v3.1.1
           imagePullPolicy: "Always"
           resources:
             limits:
               cpu: "1"
               memory: 1Gi
             requests:
               cpu: 20m
               memory: 100Mi
           volumeMounts:
           - mountPath: /etc/localtime
             name: host-time
         volumes:
         - hostPath:
             path: /etc/localtime
             type: ""
           name: host-time
   
   
   ```

2. vi cluster-configuration.yaml

   ```shell
   ---
   apiVersion: installer.kubesphere.io/v1alpha1
   kind: ClusterConfiguration
   metadata:
     name: ks-installer
     namespace: kubesphere-system
     labels:
       version: v3.1.1
   spec:
     persistence:
       storageClass: ""        # If there is no default StorageClass in your cluster, you need to specify an existing StorageClass here.
     authentication:
       jwtSecret: ""           # Keep the jwtSecret consistent with the Host Cluster. Retrieve the jwtSecret by executing "kubectl -n kubesphere-system get cm kubesphere-config -o yaml | grep -v "apiVersion" | grep jwtSecret" on the Host Cluster.
     local_registry: ""        # Add your private registry address if it is needed.
     etcd:
       monitoring: false       # Enable or disable etcd monitoring dashboard installation. You have to create a Secret for etcd before you enable it.
       endpointIps: localhost  # etcd cluster EndpointIps. It can be a bunch of IPs here.
       port: 2379              # etcd port.
       tlsEnable: true
     common:
       redis:
         enabled: false
       openldap:
         enabled: false
       minioVolumeSize: 20Gi # Minio PVC size.
       openldapVolumeSize: 2Gi   # openldap PVC size.
       redisVolumSize: 2Gi # Redis PVC size.
       monitoring:
         # type: external   # Whether to specify the external prometheus stack, and need to modify the endpoint at the next line.
         endpoint: http://prometheus-operated.kubesphere-monitoring-system.svc:9090 # Prometheus endpoint to get metrics data.
       es:   # Storage backend for logging, events and auditing.
         # elasticsearchMasterReplicas: 1   # The total number of master nodes. Even numbers are not allowed.
         # elasticsearchDataReplicas: 1     # The total number of data nodes.
         elasticsearchMasterVolumeSize: 4Gi   # The volume size of Elasticsearch master nodes.
         elasticsearchDataVolumeSize: 20Gi    # The volume size of Elasticsearch data nodes.
         logMaxAge: 7                     # Log retention time in built-in Elasticsearch. It is 7 days by default.
         elkPrefix: logstash              # The string making up index names. The index name will be formatted as ks-<elk_prefix>-log.
         basicAuth:
           enabled: false
           username: ""
           password: ""
         externalElasticsearchUrl: ""
         externalElasticsearchPort: ""
     console:
       enableMultiLogin: true  # Enable or disable simultaneous logins. It allows different users to log in with the same account at the same time.
       port: 30880
     alerting:                # (CPU: 0.1 Core, Memory: 100 MiB) It enables users to customize alerting policies to send messages to receivers in time with different time intervals and alerting levels to choose from.
       enabled: false         # Enable or disable the KubeSphere Alerting System.
       # thanosruler:
       #   replicas: 1
       #   resources: {}
     auditing:                # Provide a security-relevant chronological set of records，recording the sequence of activities happening on the platform, initiated by different tenants.
       enabled: false         # Enable or disable the KubeSphere Auditing Log System. 
     devops:                  # (CPU: 0.47 Core, Memory: 8.6 G) Provide an out-of-the-box CI/CD system based on Jenkins, and automated workflow tools including Source-to-Image & Binary-to-Image.
       enabled: false             # Enable or disable the KubeSphere DevOps System.
       jenkinsMemoryLim: 2Gi      # Jenkins memory limit.
       jenkinsMemoryReq: 1500Mi   # Jenkins memory request.
       jenkinsVolumeSize: 8Gi     # Jenkins volume size.
       jenkinsJavaOpts_Xms: 512m  # The following three fields are JVM parameters.
       jenkinsJavaOpts_Xmx: 512m
       jenkinsJavaOpts_MaxRAM: 2g
     events:                  # Provide a graphical web console for Kubernetes Events exporting, filtering and alerting in multi-tenant Kubernetes clusters.
       enabled: false         # Enable or disable the KubeSphere Events System.
       ruler:
         enabled: true
         replicas: 2
     logging:                 # (CPU: 57 m, Memory: 2.76 G) Flexible logging functions are provided for log query, collection and management in a unified console. Additional log collectors can be added, such as Elasticsearch, Kafka and Fluentd.
       enabled: false         # Enable or disable the KubeSphere Logging System.
       logsidecar:
         enabled: true
         replicas: 2
     metrics_server:                    # (CPU: 56 m, Memory: 44.35 MiB) It enables HPA (Horizontal Pod Autoscaler).
       enabled: false                   # Enable or disable metrics-server.
     monitoring:
       storageClass: ""                 # If there is an independent StorageClass you need for Prometheus, you can specify it here. The default StorageClass is used by default.
       # prometheusReplicas: 1          # Prometheus replicas are responsible for monitoring different segments of data source and providing high availability.
       prometheusMemoryRequest: 400Mi   # Prometheus request memory.
       prometheusVolumeSize: 20Gi       # Prometheus PVC size.
       # alertmanagerReplicas: 1          # AlertManager Replicas.
     multicluster:
       clusterRole: none  # host | member | none  # You can install a solo cluster, or specify it as the Host or Member Cluster.
     network:
       networkpolicy: # Network policies allow network isolation within the same cluster, which means firewalls can be set up between certain instances (Pods).
         # Make sure that the CNI network plugin used by the cluster supports NetworkPolicy. There are a number of CNI network plugins that support NetworkPolicy, including Calico, Cilium, Kube-router, Romana and Weave Net.
         enabled: false # Enable or disable network policies.
       ippool: # Use Pod IP Pools to manage the Pod network address space. Pods to be created can be assigned IP addresses from a Pod IP Pool.
         type: none # Specify "calico" for this field if Calico is used as your CNI plugin. "none" means that Pod IP Pools are disabled.
       topology: # Use Service Topology to view Service-to-Service communication based on Weave Scope.
         type: none # Specify "weave-scope" for this field to enable Service Topology. "none" means that Service Topology is disabled.
     openpitrix: # An App Store that is accessible to all platform tenants. You can use it to manage apps across their entire lifecycle.
       store:
         enabled: false # Enable or disable the KubeSphere App Store.
     servicemesh:         # (0.3 Core, 300 MiB) Provide fine-grained traffic management, observability and tracing, and visualized traffic topology.
       enabled: false     # Base component (pilot). Enable or disable KubeSphere Service Mesh (Istio-based).
     kubeedge:          # Add edge nodes to your cluster and deploy workloads on edge nodes.
       enabled: false   # Enable or disable KubeEdge.
       cloudCore:
         nodeSelector: {"node-role.kubernetes.io/worker": ""}
         tolerations: []
         cloudhubPort: "10000"
         cloudhubQuicPort: "10001"
         cloudhubHttpsPort: "10002"
         cloudstreamPort: "10003"
         tunnelPort: "10004"
         cloudHub:
           advertiseAddress: # At least a public IP address or an IP address which can be accessed by edge nodes must be provided.
             - ""            # Note that once KubeEdge is enabled, CloudCore will malfunction if the address is not provided.
           nodeLimit: "100"
         service:
           cloudhubNodePort: "30000"
           cloudhubQuicNodePort: "30001"
           cloudhubHttpsNodePort: "30002"
           cloudstreamNodePort: "30003"
           tunnelNodePort: "30004"
       edgeWatcher:
         nodeSelector: {"node-role.kubernetes.io/worker": ""}
         tolerations: []
         edgeWatcherAgent:
           nodeSelector: {"node-role.kubernetes.io/worker": ""}
           tolerations: []
   
   ```

   

![image-20220610154612945](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220610154612945.png)

**安装失败：**

参考解决方案：重置helmhttps://blog.csdn.net/qq_30019911/article/details/113747673

结果：未解决。





### 5. 问题

1. 断电后，k8s子节点的容器启动失败，节点状态为noready

   1. 解决方式：

      1. 重置节点服务器，重新加入k8s集群

         1. kubeadm reset

         2. 执行加入命令

            ```shell
            kubeadm join 192.168.10.100:6443 --token 6tfp14.d1uy5xy2mjy0q113 \
                --discovery-token-ca-cert-hash sha256:f0d96f0381301e0fd7a41bb1c3d04898f6f7926be3d06578dda19d2879aad26e
            ```

         3. ![image-20220610090232567](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220610090232567.png)

2. 
