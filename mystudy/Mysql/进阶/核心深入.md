# 锁

## 介绍

数据库锁设计的初衷是处理并发问题。作为多用户共享的资源，当出现并发访问的时候，数据库需要合理地控制资源的访问规则。而锁就是用来实现这些访问规则的重要数据结构。

锁大致可以分成全局锁、表级锁和行锁三类

## 全局锁

对整个数据库实例加锁。

MySQL提供了一个加全局读锁的方法，命令是 Flush tables with read lock (FTWRL)。

当需要让整个库处于只读状态的时候，可以使用这个命令，之后其他线程的以下语句会被阻塞：数据更新语句（数据的增删改）、数据定义语句（包括建表、修改表结构等）和更新类事务的提交语句。

使用场景：**做全库逻辑备份。**也就是把整库每个表都select出来存成文本。

**数据库备份**

之前做法：

通过FTWRL确保不会有其他线程对数据库做更新，然后对整个库做备份。注意，在备份过程中整个库完全处于只读状态。

- 如果你在主库上备份，那么在备份期间都不能执行更新，业务基本上就得停摆；
- 如果你在从库上备份，那么备份期间从库不能执行主库同步过来的binlog，会导致主从延迟。

如果不加锁，如果两个表有关联，如第一个表是余额表，第二个表是买课表，先减余额，再添加买课记录。如果出现余额未减时备份，此时备份的信息是没有扣减的余额，接下来备份买课表，备份完成后，发现了有买课记录。

不加锁的话，备份系统备份的得到的库不是一个逻辑时间点，这个视图是逻辑不一致的。

解决：在可重复读隔离级别下开启一个事务，即可拿到一个一致性视图。

官方自带的逻辑备份工具是mysqldump。当mysqldump使用参数–single-transaction的时候，导数据之前就会启动一个事务，来确保拿到一致性视图。而由于MVCC的支持，**这个过程中数据是可以正常更新的**。

**为什么还需要FTWRL呢？**

**一致性读是好，但前提是引擎要支持这个隔离级别。**比如，对于MyISAM这种不支持事务的引擎，如果备份过程中有更新，总是只能取到最新的数据，那么就破坏了备份的一致性。这时，我们就需要使用FTWRL命令了。**single-transaction方法只适用于所有的表使用事务引擎的库。**如果有的表使用了不支持事务的引擎，那么备份就只能通过FTWRL方法。这往往是DBA要求业务开发人员使用InnoDB替代MyISAM的原因之一。

**既然要全库只读，为什么不使用set global readonly=true的方式呢**？

readonly方式也可以让全库进入只读状态，但是有两个问题

- 一是，在有些系统中，readonly的值会被用来做其他逻辑，比如用来判断一个库是主库还是备库。因此，修改global变量的方式影响面更大，我不建议你使用。
- 二是，在异常处理机制上有差异。如果执行FTWRL命令之后由于客户端发生异常断开，那么MySQL会自动释放这个全局锁，整个库回到可以正常更新的状态。而将整个库设置为readonly之后，如果客户端发生异常，则数据库就会一直保持readonly状态，这样会导致整个库长时间处于不可写状态，风险较高。

业务的更新不只是增删改数据（DML)，还有可能是加字段等修改表结构的操作（DDL）。不论是哪种方法，一个库被全局锁上以后，你要对里面任何一个表做加字段操作，都是会被锁住的。

但是，即使没有被全局锁住，加字段也不是就能一帆风顺的，因为你还会碰到接下来我们要介绍的表级锁。

## 表级锁

MySQL里面表级别的锁有两种：一种是表锁，一种是元数据锁（meta data lock，MDL)。

### **表锁**

**表锁的语法是 lock tables … read/write。**与FTWRL类似，可以用unlock tables主动释放锁，也可以在客户端断开的时候自动释放。需要注意，lock tables语法除了会限制别的线程的读写外，也限定了本线程接下来的操作对象。

举个例子, 如果在某个线程A中执行lock tables t1 read, t2 write; 这个语句，则其他线程写t1、读写t2的语句都会被阻塞。同时，线程A在执行unlock tables之前，也只能执行读t1、读写t2的操作。连写t1都不允许，自然也不能访问其他表。

在还没有出现更细粒度的锁的时候，表锁是最常用的处理并发的方式。而对于InnoDB这种支持行锁的引擎，一般不使用lock tables命令来控制并发，毕竟锁住整个表的影响面还是太大。

### **元数据锁MDL。**

MDL不需要显式使用，在访问一个表的时候会被**自动加上**。

MDL的作用是，保证读写的正确性。**如果一个查询正在遍历一个表中的数据，而执行期间另一个线程对这个表结构做变更，删了一列，那么查询线程拿到的结果跟表结构对不上，肯定是不行的。**

当对一个表做增删改查操作的时候，加MDL读锁；当要对表做结构变更操作的时候，加MDL写锁。

- 锁之间不互斥，因此你可以有多个线程同时对一张表增删改查。
- 读写锁之间、写锁之间是互斥的，用来保证变更表结构操作的安全性。因此，如果有两个线程要同时给一个表加字段，其中一个要等另一个执行完才能开始执行。

**元数据锁可能产生的问题**

元数据锁是默认添加的，但也并不是我们就可以任意修表结构。

![image-20221103101707609](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20221103101707609.png)

SessionA和SessionB运行的时候，都会添加读锁，运行到SessionC的时候，会添加写锁，因为SessionA和SessionB还没有释放锁，因此SessionC的操作，以及SeesionC之后的操作都会被阻塞，如果这一时刻，某个表上的查询语句频繁，而且客户端有重试机制，也就是说超时后会再起一个新session再请求的话，这个库的线程很快就会爆满。此时大量请求处于阻塞状态，导致数据库负载过大甚至崩溃。

事务中的MDL锁，在语句执行开始时申请，但是语句结束后并不会马上释放，而会等到整个事务提交后再释放。

**如何安全地给小表加字段？**

1. 解决长事务，事务不提交，就会一直占着MDL锁

   在MySQL的information_schema 库的 innodb_trx 表中，你可以查到当前执行中的事务。如果你要做DDL变更的表刚好有长事务在执行，要考虑先暂停DDL，或者kill掉这个长事务。

2. 如果你要变更的表是一个热点表，虽然数据量不大，但是上面的请求很频繁，而你不得不加个字段，该怎么做呢

   在alter table语句里面设定等待时间，如果在这个指定的等待时间里面能够拿到MDL写锁最好，拿不到也不要阻塞后面的业务语句，先放弃。之后开发人员或者DBA再通过重试命令重复这个过程。

   ```sql
   ALTER TABLE tbl_name NOWAIT add column ...
   ALTER TABLE tbl_name WAIT N add column ... 
   ```

## 行级锁

MySQL的行锁是在引擎层由各个引擎自己实现的。MyISAM引擎不支持行锁。不支持行锁意味着并发控制只能使用表锁，对于这种引擎的表，同一张表上任何时刻只能有一个更新在执行，这就会影响到业务并发度。InnoDB是支持行锁的，这也是MyISAM被InnoDB替代的重要原因之一。

事务A更新了一行，而这时候事务B也要更新同一行，则必须等事务A的操作完成后才能进行更新。

### **两阶段锁**

![image-20221103104852875](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20221103104852875.png)

事务B的update语句会被阻塞，直到事务A执行commit之后，事务B才能继续执行。事务A持有的两个记录的行锁，都是在commit的时候才释放

**在InnoDB事务中，行锁是在需要的时候才加上的，但并不是不需要了就立刻释放，而是要等到事务结束时才释放。这个就是两阶段锁协议。**

**优化：**如果你的事务中需要锁多个行，要把最可能造成锁冲突、最可能影响并发度的锁尽量往后放。

举例：

1. 从顾客A账户余额中扣除电影票价；
2. 给影院B的账户余额增加这张电影票价；
3. 记录一条交易日志。

这三个操作在一个事务中进行，此时顾客B也进来购票，此时这两个事务冲突的部分就是语句2了。因为它们要更新同一个影院账户的余额，需要修改同一行数据。

根据两阶段锁协议，不论你怎样安排语句顺序，所有的操作需要的行锁都是在事务提交的时候才释放的。如果你把语句2安排在最后，比如按照3、1、2这样的顺序，那么影院账户余额这一行的锁时间就最少。这就最大程度地减少了事务之间的锁等待，提升了并发度。

### 死锁

当并发系统中不同线程出现循环资源依赖，涉及的线程都在等待别的线程释放资源时，就会导致这几个线程都进入无限等待的状态，称为死锁。

![image-20221103105837885](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20221103105837885.png)

这时候，事务A在等待事务B释放id=2的行锁，而事务B在等待事务A释放id=1的行锁。 事务A和事务B在互相等待对方的资源释放，就是进入了死锁状态。当出现死锁以后，有两种策略：

- 一种策略是，直接进入等待，直到超时。这个超时时间可以通过参数innodb_lock_wait_timeout来设置。（在InnoDB中，innodb_lock_wait_timeout的默认值是50s）
  - 设置过长，业务不允许。设置过短，可能造成非死锁的业务提前结束。
- 另一种策略是，发起死锁检测，发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。将参数innodb_deadlock_detect设置为on，表示开启这个逻辑。
  - 默认开启，常用。
  - 会产生额外负担，即循环等待。当一个事务被锁的时候，要去看它所依赖的线程有没有被别人锁住，如此循环。

如果所有事务都要更新同一行的场景：

每个新来的被堵住的线程，都要判断会不会由于自己的加入导致了死锁，这是一个时间复杂度是O(n)的操作。假设有1000个并发线程要同时更新同一行，那么死锁检测操作就是100万这个量级的。虽然最终检测的结果是没有死锁，但是这期间要消耗大量的CPU资源。因此，就会看到CPU利用率很高，但是每秒却执行不了几个事务。

**怎么解决由这种热点行更新导致的性能问题呢？**问题的症结在于，死锁检测要耗费大量的CPU资源。

**一种头痛医头的方法，就是如果你能确保这个业务一定不会出现死锁，可以临时把死锁检测关掉。**但是这种操作本身带有一定的风险，因为业务设计的时候一般不会把死锁当做一个严重错误，毕竟出现死锁了，就回滚，然后通过业务重试一般就没问题了，这是业务无损的。而关掉死锁检测意味着可能会出现大量的超时，这是业务有损的。

**另一个思路是控制并发度。**控制并发度，同一行同时最多只有10个线程在更新，那么死锁检测的成本很低，就不会出现这个问题。一个直接的想法就是，在客户端做并发控制。但是，你会很快发现这个方法不太可行，因为客户端很多。我见过一个应用，有600个客户端，这样即使每个客户端控制到只有5个并发线程，汇总到数据库服务端以后，峰值并发数也可能要达到3000。

在代码层面的设计：（使用队列应该也可以）

将一行改成逻辑上的多行来减少锁冲突。还是以影院账户为例，可以考虑放在多条记录上，比如10个记录，影院的账户总额等于这10个记录的值的总和。这样每次要给影院账户加金额的时候，随机选其中一条记录来加。这样每次冲突概率变成原来的1/10，可以减少锁等待个数，也就减少了死锁检测的CPU消耗。但是设计的逻辑可能更复杂一些。

## 锁详解

### 并发问题

并发事务访问相同记录出现的情况：

#### **写写**

多个未提交的事务对一条记录进行操作时，需要排队执行。

锁结构的两个重要属性：trx：锁与事务关联的id，is_waiting ：事务是否在等待

事务T1在改动记录之前，就生成了锁结构，如果事务T1之间有事务在操作该条记录，则is_waiting为true，等待状态，否则为false，表示可以直接操作该记录。事务T1结束后，检测是否还有在等待操作该记录的锁，如果事务T2也有一个锁结构在等待锁释放，则唤醒T2。

![image-20221108101407070](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20221108101407070.png)

注意：并不是所有的加锁都会生成锁结构，还有一种隐式锁。

#### **读写/写读**

Mysql在可重复读隔离级别下很大程度上避免了幻读现象，主要有两种方式：

方式一：读通过MVCC，写通过加锁

事务查询前生成readview，读取的数据只能是readview里面的数据。写的话通过加锁。

方式二：读写都通过加锁

业务场景：不允许读取记录的旧版本。

余额读取出来后，就不想让别的事务访问该余额，必须等当前事务提交后，其他事务才可以访问。这样在读取数据的时候也就需要对其进行加锁操作，让读操作和写操作也像写写那样执行。

总结：使用MVCC的方式读写彼此不冲突，性能较好；加锁的话需要排队执行，影响性能，适合特定场景。

幻读：值我们在一个事务中查询出来一部分数据后，其他事务又添加了一些数据，当我们再次查询时，查到的数据有最新添加的，如果使用加锁的方式来避免幻读比较麻烦，因为我们不知道要给谁加锁（解决幻读使用的是Mysql的间隙锁）

#### 并发下问题处理方式

##### **一致性读**

事务通过MVCC读取的方式成为一致性读，一致性读并不会对表中的任何记录进行加锁操作，其他事务可以自由地对表中的记录进行改动。

##### **锁定读**

Mysql提供了两种锁解决**读写/写读**问题。

共享锁：S锁，事务在读取记录时，需要先获取该锁

独占锁：X锁，排它锁。事务要修改一条记录时，需要先获取该锁

S锁和S锁兼容，S锁和X锁不兼容，X锁和X锁不兼容

**读取记录前就为该记录加锁的读取方式称为锁定读。**

锁定度的两种方式：

```sql
方式一：读前加S锁，读取过程中不允许其他事务修改，但可以读取。如果其他事务想获取X锁，必须等待当前事务结束，锁释放掉。
SELECT ... LOCK IN SHARE MODE;
方式二：读前加X锁，读取过程中不允许其他事务读取和修改。如果其他事务想获取X锁或S锁，必须等待当前事务结束，锁释放掉。
SELECT ... FOR UPDATE;
```

##### 写操作

如果其他事务想获取X锁，必须等待当前事务结束，锁释放掉。

**delete：先获取这条记录的x锁，再执行delete mark操作**

**insert: 新插入一条数据，受隐式锁保护，不需要在内存中生成锁结构，隐式锁通过间接的形式提供锁的功能，本身并不是锁**

**update：**

情况1：如果修改前后，被修改的列所占的内存空间大小不变，则现在B+树中获取到该记录的位置，再获取记录的x锁，最后在原位置进行修改。

情况2：被修改的列内存空间发生改变，通过B+树获取记录，并获取X锁，删除记录，最后再插入一条记录，新插入的记录获取X锁。

正常情况下，锁一般在事务提交时释放。有特殊情况：死锁

### 多粒度锁

 一个事务在表级别进行加锁，称为表锁。

表锁分为共享锁（S）和独占锁（X）

SS不互斥，SX互斥，XX互斥。

当我们要向表添加S锁时，要保证表里面无行记录是X锁；如果要向表添加X锁时，要保证表里面无记录是X锁或S锁。

添加表级S或X锁时，可以通过遍历的方式去判断这个表中的记录有无添加S或者X锁，有的话等所有行级的S锁或X锁释放后，才可以添加表级别的S锁或X锁。

毫无疑问，遍历是非常浪费性能的，Mysql提供了意向共享锁(IS)和意向独占锁（IX）来判断表中是否有行记录添加S锁或X锁。

当我们为表中的**记录**添加S锁或X锁时，会先为这个表添加IS或IX锁，等S锁或X锁释放后，IS或IX锁释放，这样当添加表级S锁或X锁时，可以通过IS或IX锁快速判断该表中是否有记录添加S锁或X锁（IS和IX只作用于添加表级S锁或X锁时，有无记录拥有S锁或X锁的判断，不作为其他用途）

### InnoDB支持的锁

MyISAM，MEMORY,MERGE，只支持表级锁，并不支持事务。加锁是以当前会话来说，同一时刻只允许一个会话对表进行写操作，最好用在只读场景下。

InnoDB支持表级锁（独占锁和共享锁），元数据锁（MDL），行级锁。

**表级锁**

**独占锁（X）和共享锁(S)：**比较鸡肋，在特殊情况下（系统崩溃时）可能用到。不建议使用。

**元数据锁（MDL）：**在使用DDL语句更改表机构时，在Server层会使用元数据锁（MDL）。DDL语句在运行时，在一些特殊的事务中运行，在这些事务开启前，需要将当前会话中的事务提交掉。

**意向锁（IS，IX）：**使用InnoDB存储引擎的表的某些记录加S锁或X锁之前，需要先在表级添加一个IS锁或IX锁。用与在添加表级S锁或X锁时，有无记录加锁的判断。

**表级（AUTO_INC锁）：**为某列添加属性，添加属性时不需要指定该列的值，系统会赋予其递增的值。实现方式有两种：

1. 使用AUTO_INC锁

   使用该锁，添加数据时会添加一个表级别的AUTO_INC锁，执行结束后释放掉。一个事务持有AUTO_INC锁的时候，其他事务的插入语句都会被阻塞，从而保证一个语句分配的递增值时连续的。

   注意：AUTO_INC锁在插入语句结束时就释放掉了，并不是事务结束的时候释放。

   适合不确定数量的插入语句。

2. 使用一个轻量级锁

   为插入语句生成AUTO_INCREMENT修饰的列的值时获取这个轻量级锁，生成AUTO_INCREMENT列的值后将锁释放，不需要等待整个插入语句执行完后释放锁。适合确定数量的插入语句。避免锁表，提高性能。

3. innodb_autoinc_lock_mode的系统变量控制使用哪一种方式来修饰AUTO_INCREMENT列的值。0：使用AUTO_INC;2:使用轻量级锁。  1：默认，二者混合。

**行级锁**

**RecordLock**：分为S锁和X锁，SS兼容，SX不兼容，XX不兼容。

**GapLock：**间隙锁，在可重复隔离级别下，使用该锁来很大程度上来解决幻读问题。

![image-20221109093224421](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20221109093224421.png)

为number为8的记录添加Gap锁之后，3-8之间添加数据是不允许立即插入的。如果要插入id为4的数据，必须等gap为8的这个事务释放掉，才可以添加。

gap锁仅作用于防止插入幻影记录。

如果想在（20，+00）添加记录，此时需要用到Mysql提供的隐藏列Supremum，即在Supremum添加Gap，防止（20，+00）之间添加幻影记录。

Infimum：页面最小的记录；Supremum：页面最大的记录

**Next-Key Lock：**既锁当前记录，又锁当前记录之前的记录（RecordLock和GapLock结合）

![image-20221109093655729](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20221109093655729.png)

**Insert Intention Lock**

一个事务在插入记录，需要判断插入位置是否已经被别的事务添加了Gap锁（Gap锁包含 Next-Key Lock），如果有，插入操作需要等待，直到Gap锁提交为止。在等待的时候在内存中也创建一个锁结构，但处于等待状态，此锁为插入意向锁。

![image-20221109094547276](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20221109094547276.png)

插入意向锁彼此之间是不会相互阻塞的，也就是T1结束后，T2和T3两个事务可以同时进行插入操作，并且插入意向锁并不会阻止别的事务继续获取该记录上任何类型的锁。

**隐式锁**

一般情况下，执行Insert语句是不会在内存中生成锁结构，（插入意向锁是有Gap的情况下生成的）而是使用隐式锁。

插入意向锁产生的问题：

插入意向锁并不阻塞其他锁，如果我插入记录成功后，事务还没有提交，其它事务是可以为此记录添加S锁或X锁，如果这样，就可能产生了脏读和脏写的情况。

发挥隐式锁作用的是通过事务id实现的，隐式锁实现：

1. 对于聚簇索引，插入一条记录后，该记录的隐藏列trx_id记录了插入当前记录的id，其它事务为该记录添加S或X锁时，会判断该记录是否在事务的执行范围内。
2. 对于非聚簇索引，没有trx_id隐藏列，但在二级索引页面的PageHeader部分有一个PAGE_MAX_TRX_ID属性，该属性代表对该页面改动的最大的事务id，如果PAGE_MAX_TRX_ID小于当前事务id，说明对该页面做修改的所有的事务都提交了，否则就需要在页面中定位到对应的二级索引记录，通过回标找到对应的聚簇索引记录，通过执行1的方式来判断。

通过判断事务id，来实现隐式锁的功能，并不需要在内存中创建所结构，但是起到的作用和加锁是一样的。

### 加锁分析

如果一个事务要获取10000条记录的锁，要生成10000个这样的锁结构就太亏了，对不同记录加锁时，如果满足以下条件，就可以放入一个锁结构。

1. 在同一个事务中加锁操作
2. 被加锁的记录在同一个页面中
3. 加锁的类型是一样的
4. 等待的状态是一样的

![image-20221109102002939](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20221109102002939.png)

有两个索引，聚簇索引，以及name列的二级索引。

#### 普通Select语句

不同隔离级别下，情况不同

1. 读未提交：不加锁，每次直接读取最新版本。有脏读，不可重复读，幻读现象。
2. 读已提交：不加锁，每次读之前生成一个ReadView。避免脏读，有不可重复读，欢度现象。
3. 可重复读：不加锁，第一次Select生成一个ReadView，把脏读，不可重复读，幻读都避免了。
   1. MVCC不能避免幻读的原因：
      1. 事务1执行查询，where number=30，没有数据，未提交事务。
      2. 事务2执行插入语句，number=30，并提交事务。
      3. 事务1更新number=30的语句，此时可以更新成功（事务2已经提交，改动并不会阻塞），此时事务1再去查num=30就可以查到了。
      4. 因此，MVCC并不能完全禁止幻读，可重复读隔离级别下只是很大程度避免了可重复读，但不是完全避免。
4. SERIALIZABLE:
   1. autocommit=0（手动提交），普通的Select语句会编程Select..LOCK IN SHARE MODE
   2. autocommit=0（自动提交），普通Select语句不会加锁，利用MVCC生成一个ReadView读取记录，自动提交只有一条记录，不会出现可重复读，幻读现象。

#### 锁定读语句

参考Mysql是怎样运行的。























# 事务隔离

在RR隔离级别下，事务开启的时候会创建一个视图，这样其他事务上对数据的修改，在当前事务中是没有影响的。

但是，如果当前事务中，要操作的记录在其他事务中正在被处理，即当前数据已被行锁，那么只能等到其他事务完成后，才能获取数据进行操作，那么此时操作的数据是其他事务修改之前的数据，还是修改之后的数据呢？

举例：

```java
mysql> CREATE TABLE `t` (
  `id` int(11) NOT NULL,
  `k` int(11) DEFAULT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB;
insert into t(id, k) values(1,1),(2,2);
```

![image-20221103141249528](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20221103141249528.png)

**事务的启动时机**

begin/start transaction 命令并不是一个事务的起点，在执行到它们之后的第一个操作InnoDB表的语句（第一个快照读语句），事务才真正启动。如果你想要马上启动一个事务，可以使用start transaction with consistent snapshot 这个命令。Mysql默认autocommit=1，表示自动提交。

事务C没有显式地使用begin/commit，表示这个update语句本身就是一个事务，语句完成的时候会自动提交。事务B在更新了行之后查询; 事务A在一个只读事务中查询，并且时间顺序上是在事务B的查询之后。

预期结果：A：1，B：2

实际结果：A：1，B：3

原因：

Mysql的两个视图：视图没有物理结构，作用是事务执行期间用来定义“我能看到什么数据”。

1. 一个是view。它是一个用查询语句定义的虚拟表，在调用的时候执行查询语句并生成结果。创建视图的语法是create view ... ，而它的查询方法与表一样。
2. 另一个是InnoDB在实现MVCC时用到的一致性读视图，即consistent read view，用于支持RC（Read Committed，读提交）和RR（Repeatable Read，可重复读）隔离级别的实现。

**快照**

在可重复读隔离级别下，事务在启动的时候就“拍了个快照”。注意，这个快照是基于整库的。

快照实现：InnoDB里面每个事务有一个唯一的事务ID，叫作transaction id。它是在事务开始的时候向InnoDB的事务系统申请的，是按申请顺序严格递增的。

每行数据也都是有多个版本的。每次事务更新数据的时候，都会生成一个新的数据版本，并且把transaction id赋值给这个数据版本的事务ID，记为row trx_id。

数据表中的一行记录，其实可能有多个版本(row)，每个版本有自己的row trx_id。

![image-20221103143632523](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20221103143632523.png)

虚线框里是同一行数据的4个版本，当前最新版本是V4，k的值是22，它是被transaction id 为25的事务更新的，因此它的row trx_id也是25。

图2中的三个虚线箭头，就是undo log；而V1、V2、V3并不是物理上真实存在的，而是每次需要的时候根据当前版本和undo log计算出来的。比如，需要V2的时候，就是通过V4依次执行U3、U2算出来。

InnoDB是怎么定义那个“100G”的快照的：

按照可重复读的定义，一个事务启动的时候，能够看到所有已经提交的事务结果。但是之后，这个事务执行期间，其他事务的更新对它不可见。

一个事务只需要在启动的时候声明说，“以我启动的时刻为准，如果一个数据版本是在我启动之前生成的，就认；如果是我启动以后才生成的，我就不认，我必须要找到它的上一个版本”。如果“上一个版本”也不可见，那就得继续往前找。还有，如果是这个事务自己更新的数据，它自己还是要认的。

在实现上， InnoDB为每个事务构造了一个数组，用来保存这个事务启动瞬间，当前正在“活跃”的所有事务ID。“活跃”指的就是，启动了但还没提交。

数组里面事务ID的最小值记为低水位，当前系统里面已经创建过的事务ID的最大值加1记为高水位。

这个视图数组和高水位，就组成了当前事务的一致性视图（read-view）。

而数据版本的可见性规则，就是基于数据的row trx_id和这个一致性视图的对比结果得到的。

这个视图数组把所有的row trx_id 分成了几种不同的情况。

![image-20221103144357265](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20221103144357265.png)



当前**事务的启动瞬间**来说，一个数据版本的row trx_id，有以下几种可能：

1. 如果落在绿色部分，表示这个版本是已提交的事务或者是当前事务自己生成的，这个数据是可见的；
2. 如果落在红色部分，表示这个版本是由将来启动的事务生成的，是肯定不可见的；
3. 如果落在黄色部分，那就包括两种情况
    若 row trx_id不在数组中，表示这个版本是已经提交了的事务生成的，可见。

如果有一个事务，它的低水位是18，那么当它访问这一行数据时，就会从V4通过U3计算出V3，所以在它看来，这一行的值是11。

![image-20221103145245780](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20221103145245780.png)

有了事务id后，系统里面随后发生的更新，就跟这个事务看到的内容无关。因为之后的更新，生成的版本一定属于上面的2或者3(a)的情况，而对它来说，这些新的数据版本是不存在的，所以这个事务的快照，就是“静态”的了。

**InnoDB利用了“所有数据都有多个版本”的这个特性，实现了“秒级创建快照”的能力。**



## MVCC原理

在Mysql中，当我们对某一条记录执行更新语句时，会生成一条undo日志，这条日志中记录了当前更新的事务id以及上一条undo日志的地址（roll_pointer）,所有的事务版本通过roll_pointer形成了一条链表，称为版本链。版本链的头结点是当前记录的最新值，每个版本中都有对应的事务id。可以通过这个版本链来控制并发事务**访问（查询）**相同记录时的行为，这就是MVCC（多版本并发控制）

具体控制方式：

在事务中，当执行Select语句时，会创建一个ReadView（RR是第一次Select创建，RC是每次Select创建），记录了当前事务id，当前活跃的事务id，当前活跃事务的最小id，当前活跃事务的最大id+1。查询时，会拿着ReadView中记录的事务信息，去版本链中拿undo日志查询。

查询时，会判断当前事务id和undo日志记录中最新的事务id，如果当前事务id和undo事务id相等，说明这条undo日志是在当前事务中产生的，可以读取到记录信息。当undo的事务id与当前事务id不一样时，此时会判断undo的事务id和ReadView记录的最小值，如果undo的事务id比ReadView记录的最小值小，说明这条undo日志是在当前事务开始前提交的，可以读取这条事务信息，如果undo的事务id比ReadView记录的最大值+1大，说明这条事务是当前事务之后开启的，不能读取，就读取rollpointer指向的undo日志信息再进行判断。如果undo的事务id在ReadView记录的最大与最小的范围内，则判断ReadView有没有记录这个事务id，如果记录，说明当前事务开启时，undo事务还没有结束，不能读取。如果没有记录，说明当前ReadView创建时，那个事务已经提交了，可以读取到。

### 问题

在开始阅读本文之前，可以先思考一下下面两个问题。

1. 众所周知，MySQL 有四大特性：ACID，其中 D 指的是持久性（Durability），它的含义是 MySQL 的事务一旦提交，它对数据库的改变是永久性的，即数据不会丢失，那么 MySQL 究竟是如何实现的呢？
2. MySQL 数据库所在服务器宕机或者断电后，会出现数据丢失的问题吗？如果不丢失，它又是如何来实现数据不丢失的呢？

在 MySQL 5.5 以后，默认的存储引擎为 InnoDB，且只有 InnoDB 引擎支持事务和数据崩溃恢复，因此本文所有内容均是基于 InnoDB 存储引擎为前提。

# 日志

## redo log

MySQL 在更新数据时，为了减少磁盘的随机 IO，因此并不会直接更新磁盘上的数据，而是先更新 [Buffer](https://so.csdn.net/so/search?q=Buffer&spm=1001.2101.3001.7020) Pool 中缓存页的数据，等到合适的时间点，再将这个缓存页持久化到磁盘。而 Buffer Pool 中所有缓存页都是处于内存当中的，当 MySQL 宕机或者机器断电，内存中的数据就会丢失，因此 MySQL 为了防止缓存页中的数据在更新后出现数据丢失的现象，引入了 redo log 机制。

当进行增删改操作时，MySQL 会在更新 Buffer Pool 中的缓存页数据时，会记录一条对应操作的 redo log 日志，这样如果出现 MySQL 宕机或者断电时，如果有缓存页的数据还没来得及刷入磁盘，那么当 MySQL 重新启动时，可以根据 redo log 日志文件，进行数据重做，将数据恢复到宕机或者断电前的状态，保证了更新的数据不丢失，因此 redo log 又叫做重做日志。它的本质是保证事务提交后，更新的数据不丢失。

与 binlog 不同的是，redo log 中记录的是物理日志，是 InnoDB 引擎记录的，而 binlog 记录的是逻辑日志，是 MySQL 的 Server 层记录的。什么意思呢？binlog 中记录的是 SQL 语句（实际上并不一定为 SQL 语句，这与 binlog 的格式有关，如果指定的是 STATEMENT 格式，那么 binlog 中记录的就是 SQL 语句），也就是逻辑日志；而 redo log 中则记录的是对磁盘上的某个表空间的某个数据页的某一行数据的某个字段做了修改，修改后的值为多少，它记录的是对物理磁盘上数据的修改，因此称之为物理日志。

redo log 日志文件是持久化在磁盘上的，磁盘上可以有多个 redo log 文件，MySQL 默认有 2 个 redo log 文件，每个文件大小为 48MB，这两个文件默认存放在 MySQL 数据目录的文件夹下，这两个文件分别为 ib_logfile0 和 ib_logfile1。（本人电脑上安装的 MySQL 时，指定存放数据的目录是：/usr/local/mysql/data，因此这两个 redo log 文件所在的磁盘路径分别是：/usr/local/mysql/data/ib_logfile0 和/usr/local/mysql/data/ib_logfile1）。可以通过如下命令来查看 redo log 文件相关的配置。

```shell
show variables like 'innodb_log%'
1
```

查询结果如图。

![image-20221110151231165](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20221110151231165.png)

1. innodb_log_files_in_group 表示的是有几个 redo log 日志文件。
2. innodb_log_file_size 表示的是每个 redo log 日志文件的大小为多大。
3. innodb_log_group_home_dir 表示的是 redo log 文件存放的目录，在这里./表示的是相对于 MySQL 存放数据的目录，这些参数可以根据实际需要自定义修改。

### redo log buffer

当一条 SQL 更新完 Buffer Pool 中的缓存页后，就会记录一条 redo log 日志，前面提到了 redo log 日志是存储在磁盘上的，那么此时是不是立马就将 redo log 日志写入磁盘呢？显然不是的，而是先写入一个叫做 redo log buffer 的缓存中，redo log buffer 是一块不同于 buffer pool 的内存缓存区，在 MySQL 启动的时候，向内存中申请的一块内存区域，它是 redo log 日志缓冲区，默认大小是 16MB，由参数 innodb_log_buffer_size 控制（前面的截图中可以看到）。

redo log buffer 内部又可以划分为许多 redo log block，每个 redo log block 大小为 512 字节。我们写入的 redo log 日志，最终实际上是先写入在 redo log buffer 的 redo log block 中，然后在某一个合适的时间点，将这条 redo log 所在的 redo log block 刷入到磁盘中。

这个合适的时间点究竟是什么时候呢？

1. MySQL 正常关闭的时候；
2. MySQL 的后台线程每隔一段时间定时的讲 redo log buffer 刷入到磁盘，默认是每隔 1s 刷一次；
3. 当 redo log buffer 中的日志写入量超过 redo log buffer 内存的一半时，即超过 8MB 时，会触发 redo log buffer 的刷盘；
4. 当事务提交时，根据配置的参数 innodb_flush_log_at_trx_commit 来决定是否刷盘。如果 innodb_flush_log_at_trx_commit 参数配置为 0，表示事务提交时，不进行 redo log buffer 的刷盘操作；如果配置为 1，表示事务提交时，会将此时事务所对应的 redo log 所在的 redo log block 从内存写入到磁盘，同时调用 fysnc，确保数据落入到磁盘；如果配置为 2，表示只是将日志写入到操作系统的缓存，而不进行 fysnc 操作。（进程在向磁盘写入数据时，是先将数据写入到操作系统的缓存中：os cache，再调用 fsync 方法，才会将数据从 os cache 中刷新到磁盘上）

### 如何保证数据不丢失

前面介绍了 redo log 相关的基础知识，下面来看下 MySQL 究竟是如何来保证数据不丢失的。

1. MySQL Server 层的执行器调用 InnoDB 存储引擎的数据更新接口；
2. 存储引擎更新 Buffer Pool 中的缓存页，
3. 同时存储引擎记录一条 redo log 到 redo log buffer 中，并将该条 redo log 的状态标记为 prepare 状态；
4. 接着存储引擎告诉执行器，可以提交事务了。执行器接到通知后，会写 binlog 日志，然后提交事务；
5. 存储引擎接到提交事务的通知后，将 redo log 的日志状态标记为 commit 状态；
6. 接着根据 innodb_flush_log_at_commit 参数的配置，决定是否将 redo log buffer 中的日志刷入到磁盘。

将 redo log 日志标记为 prepare 状态和 commit 状态，这种做法称之为两阶段事务提交，它能保证事务在提交后，数据不丢失。为什么呢？redo log 在进行数据重做时，只有读到了 commit 标识，才会认为这条 redo log 日志是完整的，才会进行数据重做，否则会认为这个 redo log 日志不完整，不会进行数据重做。

例如，如果在 redo log 处于 prepare 状态后，buffer pool 中的缓存页（脏页）也还没来得及刷入到磁盘，写完 biglog 后就出现了宕机或者断电，此时提交的事务是失败的，那么在 MySQL 重启后，进行数据重做时，在 redo log 日志中由于该事务的 redo log 日志没有 commit 标识，那么就不会进行数据重做，磁盘上数据还是原来的数据，也就是事务没有提交，这符合我们的逻辑。

实际上要严格保证数据不丢失，必须得保证 innodb_flush_log_at_trx_commit 配置为 1。

如果配置成 0，则 redo log 即使标记为 commit 状态了，由于此时 redo log 处于 redo log buffer 中，如果断电，redo log buffer 内存中的数据会丢失，此时如果恰好 buffer pool 中的脏页也还没有刷新到磁盘，而 redo log 也丢失了，所以在 MySQL 重启后，由于丢失了一条 redo log，因此就会丢失一条 redo log 对应的重做日志，这样断电前提交的那一次事务的数据也就丢失了。

如果配置成 2，则事务提交时，会将 redo log buffer（实际上是此次事务所对应的那条 redo log 所在的 redo log block ）写入磁盘，但是操作系统通常都会存在 os cache，所以这时候的写只是将数据写入到了 os cache，如果机器断电，数据依然会丢失。

而如果配置成 1，则表示事务提交时，就将对应的 redo log block 写入到磁盘，同时调用 fsync，fsync 会将数据强制从 os cache 中刷入到磁盘中，因此数据不会丢失。

从效率上来说，0 的效率最高，因为不涉及到磁盘 IO，但是会丢失数据；而 1 的效率最低，但是最安全，不会丢失数据。2 的效率居中，会丢失数据。在实际的生产环境中，通常要求是的是“双 1 配置”，即将 innodb_flush_log_at_trx_commit 设置为 1，另外一个 1 指的是写 binlog 时，将 sync_binlog 设置为 1，这样 binlog 的数据就不会丢失（后面的文章中会分析 binlog 相关的内容）。

### 疑惑

看到这里，有人可能会想，既然生产环境一般建议将 innodb_flush_log_at_trx_commit 设置为 1，也就是说每次更新数据时，最终还是要将 redo log 写入到磁盘，也就是还是会发生一次磁盘 IO，而我为什么不直接停止使用 redo log，而在每次更新数据时，也不要直接更新内存了，直接将数据更新到磁盘，这样也是发生了一次磁盘 IO，何必引入 redo log 这一机制呢？

首先引入 redo log 机制是十分必要的。因为写 redo log 时，我们将 redo log 日志追加到文件末尾，虽然也是一次磁盘 IO，但是这是顺序写操作（不需要移动磁头）；而对于直接将数据更新到磁盘，涉及到的操作是将 buffer pool 中缓存页写入到磁盘上的数据页上，由于涉及到寻找数据页在磁盘的哪个地方，这个操作发生的是随机写操作（需要移动磁头），相比于顺序写操作，磁盘的随机写操作性能消耗更大，花费的时间更长，因此 redo log 机制更优，能提升 MySQL 的性能。

从另一方面来讲，通常一次更新操作，我们往往只会涉及到修改几个字节的数据，而如果因为仅仅修改几个字节的数据，就将整个数据页写入到磁盘（无论是磁盘还是 buffer pool，他们管理数据的单位都是以页为单位），这个代价未免也太了（每个数据页默认是 16KB），而一条 redo log 日志的大小可能就只有几个字节，因此每次磁盘 IO 写入的数据量更小，那么耗时也会更短。
综合来看，redo log 机制的引入，在提高 MySQL 性能的同时，也保证了数据的可靠性。





