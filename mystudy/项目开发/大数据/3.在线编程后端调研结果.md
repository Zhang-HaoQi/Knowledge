# 在线编程调研

## 总体流程：

1. 前端提交代码
   1. 提交文本或者文件夹
2. 后端生成文件并上传到云存储
3. 将要执行的任务入队列
4. 使用k8s的job控制器生成一次性pod
   1. pod的最大生命周期为XXX秒
   2. 最大内存限制
   3. cpu限制
   4. 挂载云存储
   5. pod启动的时候回调，更新代码执行状态：运行中
   6. pod启动完成后，直接进行编译运行代码，代码结果写入日志文件中
   7. pod关闭时回调，更新代码执行状态：执行完成/关闭错误
   8. 读取日志文件，获取运行结果
5. 删除job控制器
6. 运行结果返回给前端

## **job配置文件**

```yaml
apiVersion: batch/v1
kind: Job
metadata:     # job配置
  name: job01 # 名称
  labels:     # 标签
    app: job01
  namespace: default # 命名空间
spec:  # job运行配置
#  ttlSecondsAfterFinished: 30 # 容器所有任务运行完30s后删除pod
  activeDeadlineSeconds: 100 # 容器最大的执行时间，超过这个时间后会自动删除job（如果是到了这个时间，容器关停，Job 的 Status 将变为 type:Failed 、 reason: DeadlineExceeded）。
#  backoffLimit: 6 # job启动失败后，会重新启动pod（此处重新启动指新建），7次（一次最开始创建，六次重启）之后，则不再重建
  template: # pod模板
    metadata: # pod配置
      name: job01 # pod名称
      labels: # pod标签
        job01: job01
    spec:  # pod的运行配置
      nodeName: k8s-node1 # 运行节点：后续可以取消
      restartPolicy: Never  # 重启策略：never 不重启
      containers:  # 容器配置
      - name: java-env # 容器名称
        image: dquintela/openjdk-8-jdk-alpine # 容器镜像
        imagePullPolicy: IfNotPresent # 镜像拉取策略，不存在即拉取
        command: ['sh', '-c'] # 容器创建时，执行脚本（注意：是容器创建的时候）
        args: ['cd /write_dir;javac HelloWorld.java;java HelloWorld;sleep 30'] # 进入相应文件夹并执行结果
        lifecycle: # 声明周期
          postStart: # 容器创建时候的回调
            httpGet:
             host: 192.168.58.3
             path: /api/code/test?podName=zhansgan
             port: 8080
             scheme: HTTP
          preStop: # 容器结束时候的回调
            httpGet:
             host: 192.168.58.3
             path: /api/code/test?podName=zhansgan
             port: 8080
             scheme: HTTP
        volumeMounts: # 容器挂载的目录
        - name: filedata
          mountPath: /write_dir
        resources: # 容器资源配置
            limits: # 最大的cpu和内存
              cpu: "1" # 设置
              memory: "100Mi"
            requests: # 容器至少满足cpu和内存
              cpu: "200m"
              memory: "50Mi"
      volumes: # 挂载的宿主机路径
        - name: filedata
          hostPath:
            path: /files/k8s/pro-file
```

## **配置Java Api**

```java
        
    
       <dependency>
            <groupId>io.kubernetes</groupId>
            <artifactId>client-java</artifactId>
            <version>9.0.2</version>
        </dependency>
            
            
                <dependency>
            <groupId>org.projectlombok</groupId>
            <artifactId>lombok</artifactId>
            <version>1.18.0</version>
        </dependency>

        <!-- https://mvnrepository.com/artifact/org.slf4j/slf4j-api -->
        <dependency>
            <groupId>org.slf4j</groupId>
            <artifactId>slf4j-api</artifactId>
            <version>1.7.25</version>
        </dependency>

        <!-- https://mvnrepository.com/artifact/ch.qos.logback/logback-classic -->
        <dependency>
            <groupId>ch.qos.logback</groupId>
            <artifactId>logback-classic</artifactId>
            <version>1.2.3</version>
        </dependency>

        <dependency>
            <groupId>com.alibaba</groupId>
            <artifactId>fastjson</artifactId>
            <version>1.2.49</version>
        </dependency>
```

## 注入bean

主节点获取config文件，放到类路径下，用于链接k8s

![image-20220621090739578](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220621090739578.png)

![image-20220621090758225](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220621090758225.png)

```java
@org.springframework.context.annotation.Configuration
public class K8sConfig {


    @Bean
    public ApiClient initClient() throws IOException {
        String kubeConfigPath = "classpath:config";
//        Reader rd =new FileReader(kubeConfigPath);
        //加载k8s,config
        ApiClient client =
                ClientBuilder.kubeconfig(KubeConfig.loadKubeConfig(new FileReader(ResourceUtils.getFile(kubeConfigPath)))).build();
        //将加载config的client设置为默认的client
        Configuration.setDefaultApiClient(client);
        return client;
    }
}
```

## 创建job

job控制器，用于执行一次性任务

```java
    public static void createJobPod(String filePath,String namespace) throws IOException, ApiException {
        BatchV1Api batchV1Api = new BatchV1Api();
        //加载配置文件
        File file = ResourceUtils.getFile(filePath);

        //对象映射
        V1Job v1Job = (V1Job) Yaml.load(file);
//        BatchV1Api batchV1Api = new BatchV1Api();
        V1Job job = batchV1Api.createNamespacedJob(namespace, v1Job, null, null, null);
        log.info("V1Job执行完成的格式：{}", Yaml.dump(job));
    }
```

## 获取job名称和状态

通过label进行pod的筛选，注意：pod配置的名称和生成的名称不一样，通过给不同的pod设置不同的label，通过label进行筛选，原则上筛选下来只会有一个pod

```java
 //获取pod名称
    public static String getPodRealName(String namespace, String label) throws ApiException {
        CoreV1Api coreV1Api = new CoreV1Api();
        V1PodList list = coreV1Api.listNamespacedPod(namespace, null, null, null, null, label, null, null, null, null);
        if (list != null) {
            List<V1Pod> items = list.getItems();
            if (items != null && items.size() != 0) {
                //获取pod名称
                V1Pod pod = items.get(0);
                String podName = pod.getMetadata().getName();
                //获取pod状态
                V1PodStatus status = pod.getStatus();
                System.out.println("podStatus:" + status);
                //获取容器
                List<V1Container> containers = pod.getSpec().getContainers();
                if (containers!=null&&containers.size()!=0){
                    V1Container v1Container = containers.get(0);
                }
                return pod.getMetadata().getName();
            }
        }
        return "";
    }
```

## 创建命名空间

```java
/**
     * 创建命名空间  创建namespace
     */
    public static void createNamespace() {
        CoreV1Api api = new CoreV1Api();
        V1Namespace body = new V1Namespace();
        String pretty = null;
        String dryRun = null;
        body.setApiVersion("v1");
        body.setKind("Namespace");
        V1ObjectMeta metadata = new V1ObjectMeta();
        metadata.setName("compile-test");
        Map<String, String> labelMap = new HashMap<>();
        labelMap.put("app", "compile");
        metadata.setLabels(labelMap);
        body.setMetadata(metadata);
        try {
            V1Namespace v1Namespace = api.createNamespace(body, null, pretty, dryRun);
            System.out.println("创建空间" + JSONObject.toJSONString(v1Namespace));
        } catch (ApiException e) {
            log.error("Exception when calling CoreV1Api#createNamespace");
            log.error("Status code: {}", e.getCode());
            log.error("Reason: {}", e.getResponseBody());
            log.error("Response headers: {}", e.getResponseHeaders());
        }
    }
```

## 详细api查看官方文档

[java/WatchExample.java at master · kubernetes-client/java (github.com)](https://github.com/kubernetes-client/java/blob/master/examples/examples-release-15/src/main/java/io/kubernetes/client/examples/WatchExample.java)

## 其他

[mystudy/服务器部署/虚拟机相关操作.md · Zhang-HaoQi/Knowledge - 码云 - 开源中国 (gitee.com)](https://gitee.com/zhang-haoqi/knowledge/blob/develop/mystudy/服务器部署/虚拟机相关操作.md)

## 储存与配置

### 网络储存—— nfs 安装配置及挂载

#### 安装NFS

首先我们在k8s的3台服务器上都安装[nfs](https://so.csdn.net/so/search?q=nfs&spm=1001.2101.3001.7020)服务,执行命令如下：

```shell
#所有机器安装
yum install -y nfs-utils
```

在主节点进行如下的操作：

```shell
echo "/nfs/data/ *(insecure,rw,sync,no_root_squash)" > /etc/exports
```

 上面的命令表示我们准备在**master节点暴露/nfs/data这个目录,而且是以非安全的方式，读写的权限**暴露出去

 在master节点创建要暴露的文件夹：

```java
mkdir -p /nfs/data
```

启动rpc远程绑定同步目录服务,并且是开机自启：

```shell
systemctl enable rpcbind --now
```

下面启动nfs服务器并且使配置生效：

```shell
systemctl enable nfs-server --now
#配置生效
exportfs -r
```

 使用nfs命令检查暴露的目录：

```java
exportfs
```

**在从节点进行如下的操作，让从节点同步主节点的目录：**

在两个从节点使用如下命令进行检查主节点提供了哪些目录可以同步的，是一个检查命令

```shell
showmount -e 192.168.10.3
```

 下面在两个从节点创建挂载目录：

```shell
#执行以下命令挂载 nfs 服务器上的共享目录到本机路径 /nfs/data
mkdir -p /nfs/data
```

上面的/nfs/data目录不一定也要和主节点一样,可以是/nfs/test都行

下面使用挂载命令，同步主节点的目录：

```shell
mount -t nfs 192.168.10.3:/nfs/data /nfs/data
```

上面的192.168.10.3是主节点的私有ip地址

**下面进行测试**

 首先我们在master节点的/nfs/data/目录下创建文件

![img](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/cfc790b2b13147e7b027a78b0fb409a2.png)

上面可以发现主节点我们创建了test111文件，下面我们到从节点查看：

![](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220620170422615.png)

**下面使用创建一个Deployment来测试挂载nginx**
exampledeployfornfs.yml

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: exampledeployfornfs
spec:
  replicas: 2
  selector:
    matchLabels:
      example: examplefornfs
  template:
    metadata:
      labels:
        example: examplefornfs
    spec:
      containers:
      - name: containerfornfs
        image: busybox
        imagePullPolicy: IfNotPresent
        command: ['sh','-c']
        args: ['echo"The host is ${hostname}">> /dir/data;sleep 3600']
        volumeMounts:
        - name: nfsdata
          mountPath: /dir
      volumes:
      - name: nfsdata
        nfs:
          path: /nfs/data/testDir
          server: 192.168.10.3
```

**我们需要在/nfs/data目录下提前将testDir目录建好**

**kubectl apply -f deploty-nginx.yaml**

创建后可以通过$ kubectl get deploy命令查看启动状态

![image-20220620173808234](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220620173808234.png)

接下来，验证这两个Pod是否都读写了同一个存储卷上的同一个文件。在本例中，我们先读取第一个Pod，即exampledeployfornfs-87896f598-2fnrq，使用以下命令进入Pod内部的命令界面。

```shell
kubectl exec -ti exampledeployfornfs-7d78b97455-n2l9d -- /bin/sh
```

接下来，执行以下命令，输出在存储卷中写入的文件内容。

```shell
 cat /dir/data
```

可以看到，由Deployment控制器生成的两个Pod都已经成功地将信息写入同一个存储卷的同一个文件中

![image-20220621090513334](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220621090513334.png)

其实不管哪个Pod，它们都直接引用NFS服务器上的文件，在所有的编辑操作中也都直接处理NFS服务器上的文件

由于网络存储卷使用的是不同于Kubernetes的额外系统，因此从使用角度来说，网络存储卷存在两个问题。

- 存储卷数据清理问题，需要人工清理。

- 在Pod模板中需要配置所使用存储的细节参数，于是与所使用的存储方案产生高度耦合。若基础设施和应用配置之间没有分离，则

不利于维护。

### 持久储存卷

#### PV与PVC

PV表示持久存储卷，定义了Kubernetes集群中可用的存储资源，其中包含存储资源实现的细节，如包含如何使用NFS/iSCSI/GlusterFS/RDB/azureDisk/flocker 等资源的具体设置。

PVC表示持久存储卷的申请，是由用户发起的对存储资源的请求。申请中只包含请求资源的大小和读写访问模式，无须关注具体的资源实现细节，Kubernetes会自动为其绑定符合条件的PV。

#### PV 和 PVC 基本操作

##### 创建PV

创建 examplefornfspv.yml

```shell
apiVersion : v1
kind: PersistentVolume
metadata:
  name: examplefornfspv
spec:
  capacity:
    storage: 1Gi
  accessModes:
    - ReadwriteMany
    
  persistentVolumeReclaimPolicy: Recycle
  storageClassName: examplenfs
  nfs:
    path: /nfs/data
    server: 192.168.10.3
```

**参数解释**

> capacity：表示PV的容量，通过storage子属性可以指定占用的具体存储资源（如NFS）的大小，在本例中设定为1Gi。
>
> accessModes：定义 PV 对具体存储资源（如 NFS）的访问模式。一共有 3种访问模式，分别为ReadWriteOnce（该卷可以被单个节点以读写模式挂载），ReadOnlyMany（该卷可以被多个节点以只读模式挂载），ReadWriteMany（该卷可以被多个节点以读写模式挂载）。在本例中使用ReadWriteMany。
>
> persistentVolumeReclaimPolicy：表示当删除PVC时，PV资源的回收策略。一共有3种策略，分别为Retain（保留）、Recycle（自动回收）、Delete（自动删除）。当前只有NFS和hostPath支持Recycle策略，AWSEBS、GCE PD、Azure Disk和Cinder卷支持Delete策略。在本例中使用Recycle。
>
> storageClassName：表示PV资源的描述性分类名称，例如，可以使用“ssd”“slowdisk”等具备分类的描述性名称。后续在创建PVC时可以引用这个名称来绑定PV。
>
> nfs：表示该PV使用NFS服务器作为具体的存储资源，server和path属性为之前网络存储卷示例中配置的NFS服务器及共享目录。

接下来，执行以下命令，创建PV。

```shell
kubectl apply -f examplefornfspv.yml
```

```shell
kubectl get pv
```

![image-20220621092141077](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220621092141077.png)

可以以下命令，可以查询PV资源的详情。

```shell
 kubectl describe pv {PV名称}
```

##### 创建PVC

编写文件examplefornfspvc.yml

```shell
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
 name: examplefornfspvc
spec:
 accessModes:
 	- ReadWriteMany
 storageClassName: "examplenfs"
 resources:
 	requests:
		storage: 500Mi
```

PVC创建完成后，可以通过以下命令查询PVC资源

```shell
kubectl get pvc
```

PVC已成功创建，其STATUS属性为

Bound，表示已成功绑定到符合PVC资源申请条件的PV上

![image-20220621094209253](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220621094209253.png)

此时如果再通过$ kubectl get pv命令查看已创建的PV，可以发现其STATUS属性由之前的Available变为Bound，CLAIM属性由空值变为刚才创建的PVC

![image-20220621094243349](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220621094243349.png)

PVC创建完成后，为了定义Pod并使用PVC引用的资源，首先，创建exampledeployforpvc.yml文件。

```shell
apiVersion: apps/v1
kind: Deployment
metadata:
 name: exampledeployforpvc
spec:
 replicas: 2
 selector:
  matchLabels:
    example: exampleforpvc
 template:
  metadata:
    labels:
      example: exampleforpvc
  spec:
    containers:
    - name: containerforpvc
      image: busybox
      imagePullPolicy: IfNotPresent
      command: ['sh', '-c']
      args: ['echo "The host is ${HOSTNAME}" > /dir/dataforpvc;sleep 3600']
      volumeMounts:
      - name: pvcdata
        mountPath: /dir
    volumes:
    - name: pvcdata
      persistentVolumeClaim:
        claimName: examplefornfspvc
```

​	本例中创建的名为containerforpvc的容器用于向存储卷写入数据，容器内的存储卷映射地址为/dir，它引用的存储卷为pvcdata。容器启动后会以追加方式（使用了echo ...>>...命令）向/dir/dataforpvc文件写入文本，这段代码中使用$(hostname)环境变量获取主机名称，对于Pod中的容器，获取到的是Pod名称。由于Deployment控制器拥有多个Pod，因此通过这种方式可在同一个文件下由多个Pod写入多行信息。

接下来，执行以下命令，创建Deployment控制器。

```shell
kubectl apply -f exampledeployforpvc.yml
```

创建后可以通过$ kubectl get deploy命令查看启动状态

![image-20220621095554468](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220621095554468.png)

执行 kubectl get pod -o wide 看到Deployment 控制器共创建了两个pod,分别位于两个不同的机器机器上

![image-20220621095859516](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220621095859516.png)

在本例中，PVC所绑定的PV引用中NFS服务器的共享目录为/nfs/data。在NFS服务器上执行$ cat /nfs/data/dataforpvc，可输出NFS共享目录下的文件内容。

![image-20220621101355431](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220621101355431.png)

#### PV的解绑与回收

在之前的示例中已经将exampledeployforpvc绑定到唯一的PV——exampledeployforpv上，如果此时再创建一个新的PVC，会发生什么情况呢？

```shell
vim examplefornfspvc2.yml
```

 ```shell
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
 name: examplefornfspvc2
spec:
 accessModes:
 - ReadWriteMany
 storageClassName: "examplenfs"
 resources:
 requests:
 storage: 500Mi
 ```

PVC创建完成后，可以通过 kubectl get pvc命令查询PVC资源。

![image-20220621102014527](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220621102014527.png)

可以看到examplefornfspvc2的STATUS属性为Pending，这表示PVC一直处于挂起状态，没有找到合适的PV资源

​	虽然examplefornfspv定义的空间为1GiB，而后面定义的两个PVC都各自只申请了500MiB的资源，但PV和PVC只能一对一绑定，不能一对多绑定，所以examplefornfspvc2无法申请到合适的PV资源。要使用examplefornfspvc2，要么再创建一个新的PV资源，要么就让之前的PVC和PV资源解除绑定

此时我们可以执行以下命令，先删除之前创建的PVC资源

```shell
 kubectl delete pvc examplefornfspvc
```

执行删除命令后，执行$ kubectl get pvc ，发现examplefornfspvc2状态有 pending 变为 Bound 

![image-20220621144334463](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220621144334463.png)

因为之前PV定义的回收策略（persistentVolumeReclaimPolicy）为Recycle，这表示自动回收，所以解绑后会清理PVC在PV上写入的内容。此时如果再执行$ cat /nfs/data/ dataforpvc，可以看到文件已不存在

![image-20220621144751064](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220621144751064.png)

如果自动回收失败，则PV的STATUS属性将变为Failed，这表示暂时无法使用。如果之前PV定义的回收策略是Retain，则删除PVC后资源不会自动回收。此时/data/k8snfs/dataforpvc文件依然存在，而PV的STATUS属性将变为Released，因此依然不能重新绑定其他PVC，除非重新创建PV。

#### StorageClass

之前介绍了PV及PVC的使用方式，从中可以发现，这是一种静态创建PV的方法，先要创建各种固定大小的PV，而这些PV都是手动创建的，过程非常麻烦。有时开发人员在申请PVC资源时，不一定有匹配条件的PV可用，这又带来了新的问题。

为了解决这类问题，Kubernetes提供了StorageClass抽象来动态创建PV，StorageClass大大简化了PV的创建过程。当申请PVC资源时，如果匹配到满足条件的StorageClass，就会自动为PVC创建对应大小的PV并进行绑定。

StorageClass是通过存储分配器（provisioner）来动态分配PV的，但是Kubernetes官方内置的存储分配器并不支持NFS，所以需要额外安装NFS存储分配器。

NFS 存储分配器的安装过程并不复杂。首先，执行以下命令，下载 NFS 存储分配器的deployment.yaml配置。

```shell
$ wget  https://raw.githubusercontent.com/Kubernetes-incubator/external-storage/master/nfs-client/deploy/deployment.yaml
```

修改文件部分配置

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nfs-client-provisioner
  # replace with namespace where provisioner is deployed
  namespace: default
spec:
  replicas: 1
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app: nfs-client-provisioner
  template:
    metadata:
      labels:
        app: nfs-client-provisioner
    spec:
      serviceAccountName: nfs-client-provisioner
      containers:
        - name: nfs-client-provisioner
          image: quay.io/external_storage/nfs-client-provisioner:latest
          volumeMounts:
            - name: nfs-client-root
              mountPath: /persistentvolumes
          env:
            - name: PROVISIONER_NAME
              value: fuseim.pri/ifs
              # nfs 服务器ip
            - name: NFS_SERVER
              value: 192.168.10.3
              #nfs 共享目录
            - name: NFS_PATH
              value: /nfs/data
      volumes:
        - name: nfs-client-root
          nfs:
          # nfs 服务器ip
            server: 192.168.10.3
           #nfs 共享目录
            path: /nfs/data

```

接下来，执行以下命令，创建NFS存储分配器的相关资源。

```yaml
 kubectl apply -f deployment.yml
```

![image-20220621165556433](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220621165556433.png)发f

发现没启动起来，使用kubectl describe deployment nfs-client-provisioner 查看详细信息

![image-20220621165702293](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220621165702293.png)

发现启动卡在启动 ScalingReplicaSet 这一步了，使用kubectl get rs 发现确实有一个rs资源

![image-20220621165826163](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220621165826163.png)

使用kubectl describe rs nfs-client-provisioner-6876d65556

![image-20220621170012681](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220621170012681.png)

发现确实error了

应该是没创建 serviceaccount 

编写 serviceaccount.yaml

```shell
apiVersion: v1
kind: ServiceAccount
metadata:
 name: nfs-client-provisioner
```

kubectl apply -f serviceAccount.yaml 查看 deploy 启动成功

![image-20220621170952303](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220621170952303.png)

如果Kubernetes集群已启用RBAC或正在运行OpenShift，则必须为NFS存储分配器授权。直接执行以下命令即可。

```shell
kubectl apply -f https://raw.githubusercontent.com/Kubernetes-incubator/external-storage/master/nfs-client/deploy/class.yaml
```

安装完成后可以创建StorageClass了。首先，创建managed-nfs-storage.yml文件。

```shell
 vim managed-nfs-storage.yml
```

然后，在文件中填入以下内容并保存文件

```shell
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
 name: managed-nfs-storage
provisioner: fuseim.pri/ifs
parameters:
 archiveOnDelete: "false"
```

该模板的主要含义如下。

> - apiVersion表示使用的API版本，storage.k8s.io/v1表示使用Kubernetes API的稳定版本。
> - kind表示要创建的资源对象，这里使用关键字StorageClass。
> - metadata中的name属性定义了当前资源的名称。
> - provisioner表示存储分配器的名称。这里需要使用之前在Deployment模板中配置的PROVISIONER_NAME，即fuseim.pri/ifs。
> - parameters表示该资源对象的参数。若archiveOnDelete为false，表示与之关联的PVC在删除时，它所绑定的PV不会被存储分配器保留；若为true，则相反。

接下来，执行`kubectl apply -f StorageClass.yml`，创建StorageClass。

![image-20220621173039156](https://mynotepicbed.oss-cn-beijing.aliyuncs.com/img/image-20220621173039156.png)

StorageClass 创建完成后就可以创建 PVC 了。首先，创建exampleforstorageclass.yml文件。

```shell
vim exampleforstorageclass.yml
```

